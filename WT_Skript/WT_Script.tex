\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english,german]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage{dirtytalk}
\usepackage{color}
\usepackage{leftidx}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{enumerate}


\usepackage[
top=2.5cm,
bottom=2.5cm,
left=2.5cm,
right=2.5cm,
headheight=15pt,
includehead,
includefoot,
heightrounded]{geometry}

\setstretch{1}

\pagestyle{fancy}
\fancyhead[C]{\rule{4\textwidth}{5\baselineskip}}
%\setlength{\headheight}{52pt}
\fancyhf{}
\rhead{\today}
\lhead{Wahrscheinlichkeitstheorie}
\chead{Phil Trommer}
\rfoot{Seite \thepage}



%\renewcommand{\thefootnote}{\roman{footnote}}

%\titleformat{\section}
%  {\normalfont\large\bfseries}
%  {\thesection}
%  {1em}
%  {\uline{#1}}  
 
\pagenumbering{arabic}
  
\newcommand{\newpara}{\vskip 0.5cm}
\newcommand*{\textcal}[1]{\textit{\fontfamily{qzc}\selectfont#1}}


\author{Phil Trommer}
\title{Wahrscheinlichkeitstheorie anhand der Vorlesung  \\von Dr. Arleta Szkola,\\ Universität Leipzig}

\begin{document}
\setlength{\parindent}{0cm}
\maketitle
\thispagestyle{empty}

\newpage

\tableofcontents

\newpage
\part{Einleitung}
	\section*{Inhalte}
	\begin{itemize}
		\item Wahrscheinlichkeitstheorie
		\item mathematische Statistik
		\item beide unter dem Begriff \textit{Stochastik} zusammengefasst
	\end{itemize}
\newpara
	\section*{Literaturempfehlungen}
	\begin{itemize}
		\item Hans-Otto-Georgii : \say{Stochastik}
	\end{itemize}
\newpara
	\section*{Ausgangsfrage:}
		Ist Zufall etwas fundamentales?
	\section*{Zentrale Frage: }
		Was ist die \textbf{WS} ( Wahrscheinlichkeit) eines zufälligen Ereignisses? \newline
		Ursprünglich existierten folgenden 2 Definitionen.
		\subsection*{(1) Frequentistische Definition: }
			Die WS eines zufälligen Ereignisses ist der Grenzwert der relativen Häufigkeit des Eintretens 						dieses Ereignisses bei vielen Wiederholungen.
		\subsection*{(2) Bayes'sche Definition: }
			Die WS eines zufälligen Ereignisses ist ein Maß dafür, wie stark man vom Eintreten dieses 							Ereignisses überzeugt ist.
	\section*{Moderner Zugang}
	\begin{itemize}
		\item Der moderne Zugang geht auf Alexander Kolmogorov zurück\\
		$\hookrightarrow$ \textit{Kolmogorov'schen Axiome}
		\begin{itemize}
			\item[- A1] Die WS eines Ereignisses ist eine reele Zahl x mit $0 \leq\ x \leq\ 1$
			\item[- A2] Das sichere Ereignis hat die WS = 1
			\item[- A3] Eine abzählbare Vereinigung sich gegenseitig ausschließender Ereignisse hat die WS 
			gleich der Summe der einzelnen WS.
			\item[-   ] Diese Axiome bilden die Grundlage der modernen Formulierung der \textbf{WT} (	 						Wahrscheinlichkeitstheorie) \newline
			\say{moderne Formulierung} \newline
			$\hookrightarrow$ basiert auf dem Konzept des \textit{W-Raumes} (Wahrscheinlichkeitsraum)			
		\end{itemize}
		\item Sei $\Omega$ (Omega) eine Menge (die Elemente heißen \textit{Elementarereignisse})
		\begin{itemize}
			\item Sei A ein geeignetes System von Teilmengen von $\Omega$ ( diese Mengen heißen 								\textit{Ereignisse})
			\item Und sei\space\space P: \textit{A} $\rightarrow$  [0,1] eine Abbildung\newline
			$\hookrightarrow$ \textit{W-Maß} (Wahrscheinlichkeitsmaß)
			\item Die Abbildung P erfüllt dabei diese Rechenregeln:
			\begin{itemize}
				\item[(i)   ] $P\left(\Omega\right)$ = 1
				\item[(ii)  ] $P(A^\complement) = 1 - P(A)$ , 
				mit $A^\complement = \Omega\setminus A$ und 
				$\forall A \in \textit{A}$
				\item[(iii) ] $P\left(\bigcup_{n\in \mathbb{N}}A_{n}\right) \leq \sum_{n\in \mathbb{N}}
				P(A_{n})$, \space\space\space $A_{n}\in \textit{A}$ 				
			\end{itemize}						
		\end{itemize}
		\item Damit heißt ($\Omega$, \textit{A} , P) ein W-Raum.
	\end{itemize}
\newpage
\part{Thematischer Vorlesungsbeginn}
\newpara
\section{Laplace - Modell}
	$\hookrightarrow$ einfachstes Modell für ein Zufallsexperiment
\newpara
	Sei $\Omega$ eine Menge
	\par
	\begingroup
	\leftskip=2cm % ggf. verstellen
		\noindent $\hookrightarrow \Omega$ ist Ergebnismenge eines Zufallsexperimentes, wenn $\Omega$ alle 
		möglichen Ausgänge des Experiments erfasst \newline
		$\hookrightarrow$ jedes $\omega\in\Omega$ ist ein mögliches Ergebnis/Ausgang welches
 		\textit{Elemtarereignis}
		
		genannt wird. 
	\par
	\endgroup
	Ist $\Omega$ eine höchstens abzählbare Menge, dann heißt jede Teilmenge E  
	von $\Omega$ ein Ereignis. \newline			
	($\forall E\in\mathcal{P}(\Omega)$ ist ein Ereignis)
	\subsection{Definition}
	Ein Zufallsexperiment heißt \textit{Laplace-Experiment} der Ordnung $N\in\mathbb{N}$, wenn die 						Ergebnissmenge $\Omega$ endlich ist mit |$\Omega$| = N und die Elementarereignisse alle 							gleichwahrscheinlich sind, d.h. die WS für das Eintreten des Elementarereignisses $\omega\in\Omega$ ist
	$\frac{1}{N}$. (Formal: 
	$P({\omega}) = \frac{1}{N}$ \space , \space $\forall\omega\in\Omega$ ) 
		\subsubsection*{Beispiel \say{Spielwürfel}}
		$\hookrightarrow$ Augenzahlen 1 - 6 entsprechen den möglichen Ausgängen des Experiments
		\par
		\begingroup
		\leftskip=2cm
			\noindent $\hookrightarrow\Omega$ = \{1,2,3,4,5,6\} Ergebnismenge \newline
			N = |$\Omega$| = 6 ist die Ordnung
		\par
		\endgroup
		Im Laplace-Modell ist die WS für das Eintreten eines Ereignisses $E\in P(\Omega)$ gegeben
		durch
		\begin{center} 
		\textbf{$P(E) = \frac{|E|}{N}$}				
		\end{center}
		Denn gemäß den \textit{A1} - \textit{A3} gilt \newline
		$P(E) =_{A3} \sum_{\omega\in E}P({\omega}) = \sum_{\omega\in E}\frac{1}{N} = 
		\frac{|E|}{N}$ \newline
		Beispiel \newline
		E: es fällt eine ungerade Augenzahl , \space\space E = \{1,3,5\} = $\{1\}\cup\{3\}\cup\{5\}$ 
		$$\hookrightarrow P(E) = P(\{1,3,5\}) = P(\{1\}) + P(\{3\}) +
		P(\{5\})$$ 
		$$= \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}$$ 		
\newpage
		Beispiel: \newline
		A ist ein Alphabet mit 5 Buchstaben, d.h. |A| = 5 
\newpara
		Frage: \newline
		Wie groß ist die WS, das ein zufällig gewähltes Wort der Länge 3 genau 2 verschiedene Buchstaben
		enthält? \newline
		Antwort: 
		$$\Omega = A \times A \times A$$ 
		$$|\Omega| = |A^3| = 125$$ 
		Ereignis E: Wort der Länge 3 enthält genau 2 verschiedene Buchstaben
		$$|E| = ? \quad |E| = 5 * 4 * 3 = 60 \newline$$
		$\hookrightarrow$ Möglichkeiten, um das doppelt vorkommende Symbol zu wählen \newline
		Daraus folgt $$P(E) = \frac{60}{125}$$ 
\newpara
		Beispiel: \say{Wiederholtes Werfen eines Spielwürfels} \newline
		\begin{small}
		(fair, 6-seitig)
		\end{small} \newline
		Der Würfel wird n -mal geworfen und jedes mal die Augenzahl notiert. \newline
		$\hookrightarrow$ Es handelt sich um ein Laplace-Experiment der Ordnung N = ?
		$$\hookrightarrow \Omega = \{1,...,6\}^n = \{1,...,6\} \times \{1,...,6\} \times .....$$
		$$\hookrightarrow N = |\Omega| = 6^n$$
		Für jedes $\omega = (\omega_{1},...,\omega_{1})\in\Omega$ gilt P(\{$\omega$\}) = $\frac{1}{6^n} =
		6^{-n}$
		Wir betrachten
		\begin{itemize}
			\item[a) ] $E_{i}$ : beim i-ten Wurf fällt die \say{6}
			\item[b) ] $\tilde{E}_{i}$ : nur beim i-ten Wurf fällt die 6
			\item[c) ] E : die 6 fällt genau einmal
		\end{itemize}
		Zu a)
		$$E_{i} = \{\omega=(\omega_{1},...,\omega_{n})\in\Omega | \omega_{i} = 6\}$$
		$$\Rightarrow |E_{i}| = 6^{n-i} \Rightarrow P(E_i) = \frac{6^{n-1}}{6^n} = \frac{1}{6}$$
		Zu b)
		$$\tilde{E}_i = \{\omega=(\omega_{1},...,\omega_{n})\in\Omega | \omega_i=6, w_k \neq 6,
		\forall k \neq i\}$$
		$$\Rightarrow |\tilde{E}_i| = 5^{n-i} * 1 \Rightarrow P(\tilde{E}_i) = \frac{5^{n-1}}{6^n} = 
		\frac{1}{6} * \left(\frac{5}{6}\right)^{n-1}$$
		Zu c)
		$$E = \bigcup_{i=1}\tilde{E}_i = \tilde{E}_1 \cup \tilde{E}_2 \cup ... \cup \tilde{E}_n $$
		$$\Rightarrow |E| = \sum_{i=1}^n|\tilde{E}_i| = n*5^{n-1} \Rightarrow P(E) = \frac{n}{6} *
		\left(\frac{5}{6}\right)^{n-1}$$
\newpara
\newpara
\section{Kombinatorik}
	Unser Kontext: Laplace-Experiment \newline
	$\hookrightarrow$ Formel für WS eines Ereignisses
	$$E\subset\Omega \qquad P(E)=\frac{|E|}{|\Omega|}$$
	Das Urnenmodel:
	\begin{itemize}
		\item Urne enthält endlich viele gleichartige (in Größe, Gewicht, etc.) Kugeln
		\item Die Urne ist formal eine Menge \newline
		$\hookrightarrow$ wir denken uns die Kugeln durchnummeriert
		\item Die Kugeln werden nacheinander \textit{blind} der Urne entnommen
	\end{itemize}
	Man unterscheidet die Varianten:
	\begin{itemize}
		\item Wiederholtes Ziehen mit Zurücklegen
		\item Wiederholtes Ziehen ohne Zurücklegen
	\end{itemize}
	Sowie
	\begin{itemize}
		\item mit Anordnung (unter Beachtung der Reihenfolge der Ausgänge der Einzelbeziehungen )
		\item ohne Anordnung
	\end{itemize}	
	\begin{tikzpicture}[
  	baseline,
  	level distance=35mm,
  	text depth=.5em,
  	text height=.5em,
  	bend angle = 15,
  	auto,  	
  	level 1/.style={sibling distance=18em},
  	level 2/.style={sibling distance=8em}]
  	\node (root) at (0,0) {$\hookrightarrow$ insgesamt 4 Varianten};
  	\node (with) at (-3, -2) {mit Anordnung};
  	\node (without) at (3, -2) {ohne Anordnung};
  	\node (i) at (-5, -4) {mit Zurücklegen};
  	\node (i2) at (-5, -4.5) {Var I};
  	\node (i3) at (-5, -5) {$n^k$};
  
 	 \node (ii) at (-1.75, -4) {ohne Zurücklegen};
  	\node (ii2) at (-1.75, -4.5) {Var II};
  	\node (ii3) at (-1.75, -5) {$\frac{n!}{(n-k)!}$};

  	\node (iii) at (1.75, -4) {mit Zurücklegen};
 	\node (iii2) at (1.75, -4.5) {Var III};
  	\node (iii3) at (1.75, -5) {$\binom{n}{k} = \frac{n!}{(n-k)!k!}$};

  	\node (iv) at (5, -4) {ohne Zurücklegen};
  	\node (iv2) at (5, -4.5) {Var IV};
  	\node (iv3) at (5, -5) {$\binom{n+k-1}{k}$};

  \draw (root) -- (with);
  \draw (root) -- (without);
  \draw (with) -- (i);
  \draw (with) -- (ii);
  \draw (without) -- (iii);
  \draw (without) -- (iv);
  
	\end{tikzpicture}
\newpage
	Es gilt $|U| = n$ , k ist stetig\\
	\textbf{Zu Var I :} \\
	\say{Anzahl der 7 stelligen Telefonnummern ($10^7$)} \\
	\textbf{Zu Var II :} \\
	\say{Platzierung der ersten 3 Gewinner eines Wettbewerbes mit n Teilnehmern}\\
	$=n*(n-1)(n-2)$ \\
	\textbf{Zu Var IV :} \\
	\say{Wahlergebnisse einer Wahl in Parteien \& k Wähler (mit je einer Stimme)} 
	$$\Omega=\{(a_1,....,a_k), a_i\in\bigcup, \forall i=1,...,k$$
	$$\text{und} \quad a_1\leq a_2 \leq a_3 \leq ... \leq a_k\}$$
	$\hookrightarrow$ hiermit wird die Vorgabe \say{ohne Anordnung} formuliert \newline
	Betrachte auf $\Omega$ die Abbildung $f$, die durch folgende Vorschrift definiert ist.
	$$(a_1,....,a_k)\rightarrow(a_1+0,a_2+1,a_3+2,...,a_k+k-1)$$
	Die Abbildung $f$ bildet $\Omega$ bijektiv auf $B = f(\Omega)$ ab, wobei
	$$B=\left\{(b_1,...,b_k)|b_1<b_2<...<b_k, \quad b_i\in\bigcup_{n+k-1},\forall i=1,...k\right\}$$
	Damit ist $|\Omega| = |B|$ ($\Omega$ und $B$ sind gleichmächtig). $B$ ist aber die Ergebnismenge des 
	Zufallsexperimentes der \textbf{Var III} im Urnenmodell.
	\paragraph{Wir beobachten: }
	Die Ergebnismengen von Zufallsexperimenten die jeweils eine der Varianten im 
	Urnenmodell entsprechen, lassen sich in der Form eines karthesischen Produktes von (endlichen) Mengen beziehungsweise	einer Teilmenge davon, darstellen.
	\subsection{Definition Karthesisches Produkt}
		Seien $A,B$ zwei beliebige Mengen. Dann heißt
		$$A\times B :=\{(a,b):a\in A, b\in B\}$$
		karthesisches Produkt von A und B. Ist $k>2$ und $A_1,...A_k$ Mengen so definieren wir entsprechend
		$$A_1\times ... \times A_k :=\{(a_1,...,a_k):a_i\in A_i,i=1,...,k\}$$
		Sind $A_i=A,i=1,...,k$, so schreiben wir kurz $A^k$ für $A_1\times ... \times A_k$
		Bemerkung: \newline
		$(a,b)\in A\times B$ ist ein \textit{geordnetes Paar}.
		$$\hookrightarrow (a,b) \neq  (b,a)$$
		Wichtig ist die Unterscheidung zwischen $(a,b) und \{a,b\}$ da hier $\{a,b\}=\{b,a\}$ gilt.
\newpara
	\subsection{Definition Endlichkeit}
	Eine Menge $A$ die nur endlich viele Elemente enthält, heißt \textit{endlich}. Sonst heißt sie
	\textit{unendliche Menge}. Die Anzahl der Elemente einer endlichen Menge A wird \textit{Mächtigkeit} von
	$A$ genannt und mit $|A|$ bezeichnet.
\newpara
	\subsection{Definition Mächtigkeit}
	Zwei Mengen $A,B$ heißten \textit{gleichmächtig}, wenn eine bijektive Abbildung
	$$f:A\rightarrow B$$
	existiert.
	\paragraph{Beispiel: } 
	$\mathbb{N}$ und $\mathbb{Z}$ sind gleichmächtig, da folgende Abbildung bijektiv ist	
	\[
 	f:\mathbb{N}\rightarrow\mathbb{Z},n\rightarrow f(n):=\left\{\begin{array}{lr}
 		\frac{n}{2} &,n \text{ gerade}\\
 		\frac{1-n}{2} &,n \text{ ungerade} 	
 	\end{array}\right.
 	\]
	Explizite Darstellung der Ergebnismengen der Variante I-IV
	\begin{itemize}
		\item Var I: 
		$$\{(a_1,...,a_k):a_i\in\{1,...,k\},\forall i=1,...,k\} = \{1,...,n\}^k=:\Omega_I$$
		\item Var II:
		$$\{(a_1,...,a_k)\in\{1,...,n\}^k:a_i\neq a_j \forall i,j=1,...,k\: \text{mit} \: i\neq j\} =:\Omega_{II}$$
		\item Var III:
		$$\{(a_1,...,a_k)\in\{1,...,n\}^k:a_1<a_2<...<a_k\}=:\Omega_{III}$$
		\item Var IV:
		$$\{(a_1,...,a_k)\in\{1,...,n\}^k:a_1\leq a_2\leq ...\leq a_k\}=:\Omega_{IV}$$
		\item Baumdiagramm:
	\end{itemize}		 	
	\begin{tikzpicture}[
  	baseline,
  	level distance=15mm,
  	text depth=.5em,
  	text height=.5em,
  	bend angle = 15,
  	auto,  	
  	level 1/.style={sibling distance=8em},
  	level 2/.style={sibling distance=2em}]
  	\node {$\Omega_I$}
    	child {node {1}
      		child {node {1}}
      		child {node {2}}
      		child {node {...}}
      		child {node {n}}
    	}
    	child {node {2}
    		child {node {1}}
      		child {node {2}}
      		child {node {...}}
      		child {node {n}}
       	}
       	child {node {...}
       		child {node {1}}
      		child {node {2}}
      		child {node {...}}
      		child {node {n}}
       	}
       	child {node {n}
       		child {node {1}}
      		child {node {2}}
      		child {node {...}}
      		child {node {n}}
       	};
	\end{tikzpicture}
\newpage
	\begin{itemize}
		\item Jeder der Pfade stellt einen möglichen Ausgang des Zufallsexperimentes der Variante I im 
		Urnenmodell dar.
		\item Für Varianten II - IV müssten bestimmte Pfade weggelassen bzw. miteinander identifiziert
		werden (z.B. kein doppeltes Vorkommen).
	\end{itemize}
	\subsection{Lemma Mächtigkeit des karthesischen Produktes}
	Für zwei eindliche Mengen $A,B$ gilt
	$$|A\times B| = |A|*|B|$$
	\subsection{Satz Binomialkoeffizient}
	$$\binom{n}{k}=\binom{n}{n-k}$$
	Beweis: 
	\begin{itemize}
		\item[1) ] $\binom{n}{k}=$ Anzahl der k-elementigen Teilmengen von $\{1,...,n\}$
		\item[2) ] Mit jeder k-elementigen Teilmenge wird auch ihr Komplement $T^C$ festgelegt. Es handelt sich 
		dann um eine $(n-k)$-elementige Teilmenge von $\{1,...,n\}$
		$$\hookrightarrow |\{T\subset\{1,...,n\}:|T|=k\}|=
		|\left\{T^C:T\subset\{1,...,n\}\: und \: |T^C|=k\right\}|$$		
	\end{itemize}
	1) \& 2) zusammen ergeben die Aussage $\blacksquare$
	\subsection{Binomischer Lehrsatz}
	Für $n\in\mathbb{N}$ und $a,b\in\mathbb{R}$ gilt:
	$$(a+b)^n=\sum_{k=0}^n \binom{n}{k}a^k*b^{n-k}$$
	Beweis:
	$$(a+b)^n=(a+b)(a+b)*...(a+b)$$
	Durch ausmultiplizieren erhalten wir eine Summe, bei der jeder Term ein Produkt von n Faktoren ist. Jeder
	der Faktoren kann nur die Werte \say{a} oder \say{b} annehmen. \newline
	$\hookrightarrow$ Die Terme sind von der Gestalt $a^k b^{n-k}$, wobei $k=0,...,n$
	Für jedes feste $k$ gibt es genau $\binom{n}{k}$ Möglichkeiten $\blacksquare$
\newpage
\section{Diskrete Wahrscheinlichkeitsmodelle}
	Als Verallgemeinerung des Laplace-Modells im folgendem Sinn:
	\begin{itemize}
		\item endliche oder abzählbar undendliche Ergebnismenge
		\item die Elemtarereignisse (typischerweise) nicht mehr alle \newline 
		gleichwahrscheinlich
	\end{itemize}
	\subsection{Definition Abzählbar \& Unendlich}
	Eine Menge $\Omega$ heißt abzählbar unendlich, wenn sie mit $\mathbb{N}$ gleichmächtig ist.
	\subsection{Definition Diskreter W-Raum}
	Sei $\Omega$ eine höchstens abzählbare nichtleere Menge und
	$$\rho :\Omega\rightarrow[0,1],\omega\rightarrow\rho (\omega)$$
	eine Funktion mit
	$$\sum_{\omega\in\Omega}\rho (\omega)=1$$
	Dann heißt $\rho$ eine \textit{(Zähl)-Dichte}, W-Vektor oder Gewichtsfunktion auf $\Omega$.
	Die Abbildung
	$$P: \mathcal{P}(\Omega)\rightarrow[0,1], E\mapsto P(E):=\sum_{\omega\in E}\rho(\omega)$$
	wird als W-Maß auf $\mathcal{P}(\Omega)$ genannt. $(\Omega,\mathcal{P}(\Omega),P)$ wird als 
	\textit{diskreter W-Raum} bezeichnet.
	
	\subsection{Definition Ereignisraum}
	Sei $\Omega$ eine höchstens abzählbare Ergebnismenge. Dann versteht man unter einem \textit{Ereignisraum}
	das Paar $(\Omega,\mathcal{P}(\Omega))$.
	\paragraph{Frage: }
	Warum wird $P$ als W-Maß bezeichnet?
	\begin{itemize}
		\item Der Wert $P(E)$, den $P$ dem Ereignis $E\in\mathcal{P}(\Omega)$ zuordnet, wird als 							Wahrscheinlichkeit des Ereignisses $E$ interpretiert
		\item Speziell für Elementarereignisse $\{\omega\}\in\mathcal{P}(\Omega)$ erhalten wir gemäß der
		Definition von $P$ die Relation
		$$P(\{\omega\})=\rho(\omega)$$
	\end{itemize}
	\subsection{Satz (Begründung für die Interpretation)}
	\label{3dot4}
	Ist $\rho :\Omega\rightarrow[0,1]$ eine Gewichtsfunktion, d.h. $$\sum_{\omega\in\Omega}\rho(\omega)=1,$$
	dann erfüllen die Funktionswerte $P(E),E\in\mathcal{P}(\Omega)$ des zugehörigen W-Maßes 
	$$P:\mathcal{P}(\Omega)\rightarrow[0,1],E\rightarrow P(E)=\sum_{\omega\in E}\rho(\omega)$$
	die \textit{kolmogorovschen Axiome} A1 bis A3 (siehe Einleitung).
	\paragraph{Beweis: }	 
	\begin{itemize}
		\item[zu A1) ] zu zeigen ist $0\leq P(E)\leq 1,\forall E\in\mathcal{P}(\Omega)$
		$$P(E)=^{def}\sum_{\omega\in E}\rho(\omega)\geq\sum_{\omega\in E}0$$
		$$\hookrightarrow \rho(\omega)\geq 0\: , \forall\omega\in\Omega$$
		$$P(E)=^{def}\sum_{\omega\in E}\rho(\omega)\leq\sum_{\omega\in E\cup E^C}\rho(\omega)=\sum_{\omega
		\in\Omega}=1$$
		$$\hookrightarrow \: \text{da} \: \rho(\omega)\geq 0 \: , \forall\omega\in\Omega$$
		\item[zu A2) ] $$\mathcal{P}(\Omega) = \sum_{\omega\in\Omega}\rho(\omega)=1$$
		\item[zu A3) ] Seien $E_1,...,E_n$ paarweise disjunkte Ereignisse
		$$P\left(\bigcup_{i=1}^n E_i\right)=\sum_{\omega\in\bigcup_{i=1}^n (E_i)}\rho(\omega)=
		\sum_{i=1}^n P(E_i)$$
	\end{itemize}
	\subsection{Beispiel Urne}
	Sei $U$ eine Urne mit $n$ Kugeln, d.h. $|U| = n$. Die Kugeln in $U$ besitzen ein weiteres 
	Unterscheidungsmerkmal, hier die Farbe.
	\begin{itemize}
		\item Sei $S\subset U$ die Teilmenge der schwarzen Kugeln
		\item Sei $W\subset U$ die Teilmenge der weißen Kugeln
		\item Gelte $S\cup W=M$
	\end{itemize}
	Betrachte das \textit{Zufallsexperiment} $\mathcal{E}$: \newline
	Es wird eine Kugel aus M entnommen und die Farbe notiert. \newline
	$\hookrightarrow$ Ergebnismenge von $\mathcal{E}: \Omega=\{s,w\}$ 
	\paragraph{$\rightarrow$ Frage: }	
	Wie wahrscheinlich ist es, das ich eine schwarze Kugel ziehe? 
	\paragraph{Antwort: }
	$P(E)=\frac{|S|}{M}=\tilde{P}(S)$
	\paragraph{Begründung: }
	Dem Experiment $\mathcal{E}$ liegt ein Laplace-Experiment $\tilde{\mathcal{E}}$ 
	\\zugrunde. \newline
	$\tilde{\mathcal{E}}$: Es wird eine Kugel aus M gezogen mit dem zugehörigen W-Raum 
	$$(\tilde{\Omega}=U,\mathcal{P}(U),\tilde{P}) \quad wobei \quad \tilde{P}(E) =\frac{|E|}{n}$$
	Betrachten wir jetzt die Abbildung	
	\[
 	f(\tilde{\omega})=\left\{\begin{array}{lr}
 		s &,\text{falls } \tilde{\omega}\in S\\
 		w &,\text{sonst } \widehat{=}\tilde{\omega}\in W
 	\end{array}\right.
 	\]
	Dann erhalten wir die Ergebnismenge $\Omega$ von $\mathcal{E}$ als Bildmenge von $\tilde{\Omega}=U$
	untert der Abbildung $f$, das heißt
	$$\Omega =f(\tilde{\Omega})$$
	und die Gewichtsfunktion:
	$$\rho :\Omega\rightarrow [0,1]$$
	ergibt sich aus
	$$\rho(\omega)=P(\{\omega\})=\tilde{P}(f^{-1}(\{\omega\}))=\frac{|f^{-1}(\{\omega\})|}{n}$$
	\subsection{Definition Mengen in Funktionen}
	Seien $A,B$ nichtleere Mengen und
	$$f: A \rightarrow B$$
	eine Abbildung mit dem Definitionsbereich $A$ und Bildbereich $B$. Für jedes $M\subset A$ heißt die Menge
	$$f(M)=\{f(a)\in B: \: a\in M\}\subset B$$
	Die \textit{Urbildmenge} von $S\subset B$  und $f$ ist gegeben durch
	$$f^{-1}(S):=\{a\in A: \: f(a)\in S)\}$$
	\paragraph{Beispiel: }
	$$f:\: \mathbb{R}\rightarrow\mathbb{R}\quad , x\longmapsto e^x,\quad f^{-1}([0,1])=\mathbb{R}$$ 
	 \section{Zufallsvariablen}
	 \subsection{Definition Zufallsvariable}
	 \begin{itemize}
	 	\item $(\Omega ,\mathcal{P}(\Omega^{'}),P)$ diskreter W-Raum
	 	\item $\Omega$ höchstens abzählbar \newline
	 	$\hookrightarrow X:\Omega^{'}\rightarrow\Omega$ heißen Zufallsvariablen mit Werten in $\Omega$ und 
	 	Verteilung
	 	$$P_X(E):=P(X^{-1}(E)),\:\forall E\subset\Omega$$
	 	\item Mit dem Konzept einer Zufallsvariable werden in der Wahrscheinlichkeitstheorie 
	 	Zufallsexperimente beschrieben.
	 	\item Eine Zufallsvariable mit Werten in $\Omega$ beschreibt ein Zufallsexperiment, dessen 
	 	Ergebnismenge gleich $\Omega$ oder einer Teilmenge von $\Omega$ ist. Genauer gibt $X(\Omega^{'})$
	 	die Ergebnismenge des Zufallsexperimentes an.
	 	\item Die Verteilung $P_X$ der Zufallsvariable $X$ ist formal ein W-Maß. Damit stellt das Tripel
	 	$(\Omega ,\mathcal{P}\Omega , P_X)$ einen diskreten W-Raum dar.
	 \end{itemize}
	 \subsection{Theorem}
	 Seien $(\Omega^{'},\mathcal{P}(\Omega^{'}),P)$ ein diskreter W-Raum und $X$ eine Abbildung auf 
	 $\Omega^{'}$, dann ist durch
	 $$P_X:\: P(\Omega)\rightarrow [0,1]$$
	 $$E\longmapsto P(X^{-1}(E))$$
	 wobei $\Omega=X(\Omega^{'})$ ein W-Maß auf $P(\Omega)$ definiert. 
	\paragraph{Beweis: }
	Wir konstruieren eine Gewichtsfunktion
	$$\rho :\Omega\rightarrow [0,1]$$
	so dass $\forall E\in\mathcal{P}(\Omega)$ gilt:
	$$P_X(E)=\sum_{\omega\in E}\rho(\omega)$$
	\paragraph{Unser Ansatz:}	
	$$\forall\omega\in\Omega$$
	$$\rho(\omega)=P(X^{-1}(\{\omega\})) \quad mit \quad X^{-1}(\{\omega\})\subset\Omega^{'}$$
	Da $P$ nach Voraussetzung ein W-Maß ist, gilt dann $\rho(\omega)\in[0,1]$.\newline
	Zum anderen erhalten wir
	$$\sum_{\omega\in\Omega}\rho(\omega)=^{def}
	\sum_{\omega\in\Omega}P\left(X^{-1}(\{\omega\}\right)
	=\footnote{Siehe \ref{3dot4}}P\left(\bigcup_{\omega\in\Omega}X^{-1}(\{\omega\})\right)=P(\Omega^{'})
	=1 \quad \blacksquare$$
	\subsubsection{Beispiel Augensumme}
	Zufallsexperiment $\mathcal{E}$: Es werden 2 Spielwürfel nacheinander geworfen und die Augensumme notiert.
	\paragraph{$\hookrightarrow$ Frage: }
	Was ist das geeignete W-Modell für $\mathcal{E}$? \newline
	\textbf{alternativ:} Wie sieht die Zufallsvariable $X$: \say{Augensumme von zwei 6-seitigen Würfeln} 
	formal aus?
	\paragraph{Antwort: }
	Die Ergebnismenge von $\mathcal{E}$ ist $\Omega=\{2,...,12\} \Rightarrow |\Omega|=11$
	\paragraph{Beobachtung: }
	Das Laplace-Modell scheint ungeeignet, da es mit dem Zufallsexperiment $\mathcal{E}$:
	\say{Ein Spielwürfel wird 2 mal nacheinander geworfen und die Augenzahl notiert} nicht konsistent ist.
	\newline
	Wir formulieren eine geeignete Abbildung:
	$$X:\Omega^{'}\rightarrow\Omega \quad \text{wobei} \quad \Omega^{'}=\{1,...,6\}^2\: ,\: |\Omega^{'}|=36$$
	$$\hookrightarrow (a,b)\rightarrow a+b$$
	$$\Rightarrow\:\rho(\omega)=P(\{\omega\}):=P^{'}(X^{-1}(\{\omega\}))=\frac{|X^{-1}(\{\omega\})|}{36}$$
	$$=\frac{|\{(a,b):\: a,b\in\{1,...6\}\: , \: a+b=\omega\}}{36}$$
	$$\hookrightarrow\rho(\omega)=\frac{1}{36}*(6-|\omega -7|)\: , \:\omega\in\{1,...,12\}=\Omega$$
	Die Verteilung $P_X$ ist dann das W-Maß zur Gewichtsfunktion $\rho$, die in der vorherigen Gleichung 
	festgelegt ist. Es gibt mehrere wichtige Verteilungen.
	\subsection{Definition Bernoulli-Verteilung}
	Sei $p\in[0,1]$. Die Verteilung auf $\mathcal{P}(\Omega)$, wobei $\Omega=\{0,1\}$, zur Gewichtsfunktion
	$\rho(1)=p$ und $\rho(0)=1-p$ heißt \textit{Bernoulli-Verteilung zum Parameter p} und wird mit 
	\textit{$B_p$} bezeichnet.
	\subsection{Definition Bernoulli-Experiment}
	Ein Zufallsexperiment heißt \textit{Bernoulli-Experiment}, wenn es durch eine Bernoulli -verteilte 
	Zufallsvariable beschrieben wird.\\
	Bernoulli-Experimente haben folgende Eigenschaften:
	\begin{itemize}
		\item Ergebnismenge $=\{0,1\}$
		\item Modellieren zum Beispiel das Werfen einer Münze, wobei \say{Zahl} und \say{Kopf} durch die Werte
		\say{0} bzw. \say{1} kodiert werden
		\item Allgemein interpretiert man 
		\begin{equation*}
		\begin{split}
		1 \quad & \widehat{=} \quad \text{Treffer/Erfolg}\\
		0 \quad & \widehat{=} \quad \text{kein Treffer/Misserfolg}
		\end{split}
		\end{equation*}
		\item[$\hookrightarrow$] $B_p$ ist die Bernoulli-Verteilung mit Erfolgswahrscheinlichkeit $p$
		\item Häufig steht \say{1} sinngemäßt für \say{Ja} und \say{0} für \say{Nein}, wenn die betreffende 
		Zufallsvariable das Eintreten bzw. Nicht-Eintreten eines Ereignisses beschreibt.
	\end{itemize}
	\subsection{Definition Indikatorfunktion}
	Seinen $\Omega$ eine Menge und $M\subseteq\Omega$. Die auf $\Omega$ definierte Funktion	
	\[
 	\textcal{1}_M (\omega):=\left\{\begin{array}{lr}
 		1 &,\omega\in M\\
 		0 &,\text{sonst} 
 	\end{array}\right.
 	\]
	heißt \textit{Indikatorfunktion} von M auf $\Omega$.
	\subsubsection{Beispiel}
	\paragraph{Beispiel 1}	
	Ein 6-seitiger Würfel wird geworfen und die Augenzahl \say{6} als Treffer gewertet.
	\begin{itemize}
		\item[$\hookrightarrow$] Zufallsvariable: Es wird ein Treffer erzielt ist Bernoulli-verteilt zum Parameter
		$p=\frac{1}{6}$ \newline
		$\hookrightarrow$ formal: 
		\begin{equation*}
		\begin{split}
		X:\{1,...,6\} & \rightarrow\{0,1\}\\
		\omega & \rightarrow\textcal{1}_{\{6\}}(\omega)
		\end{split}
		\end{equation*}		
	\end{itemize}
	\paragraph{Beispiel 2}
	Ein 6-seitiger Würfel wird dreimal nacheinander geworfen und jeweils die Augenzahl notiert. Ab einer 
	Augensumme =15 wird ein Gewinn ausgezahlt. 
	\begin{itemize}
		\item[$\hookrightarrow$] Zufallsvariable $Z$: Gewinnauszahlung \newline
		$\hookrightarrow$ formall: 
		$$Z=\textcal{1}_{\{X\geq 15\}}\circ X\footnote{Zufallsvariable: Augensumme nach 3 Würfen}$$
		somit $ Z:\{1,...,6\}^3\rightarrow\{0,1\}$		
	\end{itemize}
	\subsubsection{Definition}
	Seien $f:A\rightarrow B$ und $g:B\rightarrow C$ zwei Abbildungen. Dann ist die Verkettung $g\circ f$ definiert
	als die Abbildung
	$$g\circ f: \quad A\rightarrow C, \:\: a\rightarrow g\circ f(a):=g\left(f(a)\right)$$
	
	\subsection{Definition Gleichverteilte Zufallsvariablen}
	Sei $\Omega$ eine endliche Menge. Das W-Maß auf $\Omega$ mit der (konstanten) Zähldichte
	$$\rho (\omega)=\frac{1}{|\Omega |}\: , \:\: \omega\in\Omega$$
	heißt \textit{Gleichverteilung auf $\Omega$} und wird mit $\mathcal{U}_{\Omega}$ bezeichnet.
	
	\subsection{Definition Binomialverteilung}
	Seien $n\in \mathbb{N}$ und $p\in (0,1)$. Das W-Maß auf $\{0,1,...,n\}$ mit der Zähldichte
	$$\rho(k):=\binom{n}{k} p^k (1-p)^{n-k}\: ,\:\:\forall\: k=0,1,...,n$$
 	heißt \textit{Binomialverteilung zu den Parametern $n$ und $p$}. (Notation = $B_{n,p}$)
 	
 	\paragraph{Beispiel:}
 	Beim $n$-maligen Werfen eines 6-seitigen Würfels wird jedes Mal, wenn die Augenzahl \say{6} fällt, ein Treffer
 	gezählt. \newline
 	$\hookrightarrow$ Zu X: Anzahl der Treffer bei $n$ Versuchen \newline
 	X ist \textit{binomialverteilt}.
 	
 	\paragraph{Herleitung:}
 	Das Zugrunde liegende Zufallsexperiment ist \newline
 	$\mathcal{E^{'}}$: ein fairer 6-seitiger Würfel wird $n$-mal geworfen und jeweils die Augenzahl notiert.
 	$\mathcal{E^{'}}$ stellt ein Laplace-Experiment der Ordnung $N=6^n$ dar. Bezeichne $\Omega^{'}$ die 
 	Ergebnismenge, d.h. $\Omega^{'}=\{1,...,6\}^n$, dann ist $(\Omega^{'},\mathcal{P}(\Omega^{'}),
 	\mathcal{U}_{\Omega^{'}})$ der zugehörige Laplace-Raum.
 	$$X:\Omega^{'}\rightarrow\{0,1,...,n\}$$
 	$$\omega=\{\omega_1 ,...,\omega_n )\longmapsto\sum^n_{i=1}\textcal{1}_{\{6\}}(\omega_i)$$
 	Diese Abbildung stellt die betreffende Zufallsvariable $X$ dar!
 	\begin{equation*}
 	\begin{split}
 	\hookrightarrow P_X(\{k\}) & =\mathcal{U}_{\Omega^{'}}\left(X^{-1}(\{k\})\right)\\
 	& = \frac{|X^{-1}(\{k\})|}{6^n}\\
 	& = \frac{\binom{n}{k}*5^{n-k}}{6^{n-k+k}}=\binom{n}{k}\left(\frac{1}{6}\right)*\left(\frac{5}{6}\right)^{n-k}
 	\end{split}
 	\end{equation*}
 	
 	\subsection{Definition}
 	Seien $P$ ein W-Maß und $X$ eine Zufallsvariable. Wir schreiben $X~P$, wenn die Verteilung $P_X$ von $X$ 
 	gleich $P$ ist.
 	
 	\subsubsection{Hypergeometrisch verteilte Zufallsvariablen}
 	\paragraph{Beispiel}
 	In einer Urne befinden sich $N$ Kugeln, davon sind $k$ rot und die anderen $N-k$ Kugeln sind weiß. Es werden
 	$m$ Kugeln blind gezogen. \newline
	$\hookrightarrow$ Frage: Wie ist folgende Zufallsvariable $X$ verteilt? \newline
	$X$: Anzahl der roten Kugeln in der Stichprobe(vom Umfang $m$) \newline
	Wir stellen $X$ als eine Abbildung auf dem Laplace-Raum $(\Omega^{'},\mathcal{P}(\Omega^{'}),P)$ dar, der
	zum Laplace-Experiment $\mathcal{E}^{'}$ gehört. 
	\begin{itemize}
		\item[$\hookrightarrow$] $\mathcal{E}^{'}$: Es werden $m$ Kugeln blind und gleichzeitig aus der Urne 
		gezogen
		\item[$\hookrightarrow$] Ergebnismenge $\Omega^{'}$ zu $\mathcal{E}^{'}$:
		\begin{equation*}
		\begin{split}
		\Omega^{'} \: & =\: \{X\subseteq \{1,...,N\}\: : \: |X|=m \}\\
		& =\: \{(\omega_1 ,...,\omega_m ):\forall i=1,...,m \: :\omega_i \in\{1,...,N\},\:\: \\ 
		& \:\quad \text{ und }\omega_1 <\omega_2 <...<\omega_m\}
		\end{split}
		\end{equation*}
		\item[$\hookrightarrow$] $|\Omega^{'}|=\binom{N}{m}$
	\end{itemize}
	Bezeichne jetzt $K$ die Teilmenge der roten Kugeln. Die Zufallsvariable $X$ wird als Abbildung auf 
	$(\Omega^{'},\mathcal{P}(\Omega^{'}),\mathcal{U}_{\Omega^{'}})$ dargestellt.
	$$X:\omega\in\Omega^{'}\longmapsto\sum_{i=1}^{m}\textcal{1}_K (\omega_i)$$ 	
 	$\hookrightarrow$ Verteilung $P_X$ von $X$ ergibt sich wie folgt:
 	\begin{equation*}
 	\begin{split}
 	P_X(\{l\})\: & =\: \mathcal{U}_{\Omega^{'}}\left(X^{-1}(\{l\})\right),\:\forall l=0,...,min\{m,|k|\}\\
 	& =\: \frac{\binom{k}{l}*\binom{N-K}{m-l}}{\binom{N}{m}}
 	\end{split}
 	\end{equation*}
 	In folgenden Fällen ist $X^{-1}\left(\{l\}\right)=\emptyset$:
 	\begin{itemize}
 		\item $l>k$
 		\item $N-k<m-l$
 	\end{itemize}
 	$$\hookrightarrow X\left(\Omega^{'}\right)=
 	\left\{l*max\{0,mi\left(N-k\right)\}\leq l \leq min\{m,k\}\right\}$$
 	\paragraph{Bemerkung:} Seien $n\in\mathbb{N}$ und $k\in\mathbb{Z}$. Dann gilt
 	\[
    \binom{n}{k} = \left\{\begin{array}{lr}
        0 & ,k<0\\
        0 & ,k>n\\
        \frac{n\left(n-1\right)...\left(n-k+1\right)}{k!} & ,\text{sonst }
        \end{array}\right.
  	\]
  	\subsection{Definition Zähldichte}
 	Seien $\mathcal{N},m,k\in\mathbb{N}$ mit $K<\mathbb{N}$ und $m\leq\mathbb{N}$. Das W-Maß mit der \textit{Zähldichte}
 	$$\rho : \left\{0,1,....m\right\}\rightarrow[0,1], \:\: l\rightarrow
 	\frac{\binom{k}{l}\binom{n-k}{m-l}}{\binom{N}{m}}$$
 	Beispiele für Zufallsvariablen, die abzählbar unendlich viele Werte annehmen.
 	\begin{itemize}
 		\item \textit{Geometrische Verteilung}
 	\end{itemize}
 	\subsection{Definition Geometrische Verteilung}
 	Sei $p\in (0,1)$. Das W-Maß mit der Zähldichte 
 	$$k\in\mathbb{N}_0\rightarrow\rho(k):=(1-p)^k *p$$
 	heißt \textit{geometrische Verteilung} zum Parameter $p$ und wird mit $G_p$ bezeichnet.
 	\paragraph{Bemerkung:}
 	$$\sum^{\infty}_{k=0}\rho(k)=\sum^{\infty}_{k=0}(1-p)^k p\quad
 	\footnote{geometrische Reihe}$$
 	\paragraph{Beispiel:}
 	Eine Münze wird so lange geworden bis $\textit{Z}$ fällt. \\
 	$\hookrightarrow$ Zufallsvariable: $X$ Anzahl der Versuche, bis $\textit{Z}$ fällt ist geometrisch verteilt zum Parameter $p=0,5$.
 	\paragraph{Diskussion:}
 	Da es beliebig viele Versuche sein können bis $\textit{Z} (=1)$ fällt, scheint folgende Menge geeignet, in $X$ auf $\Omega$ zu modellieren.
 	$$\Omega=\left\{(x_1,x_2,x_3,...):x_i\in\{0,1\}:\forall i\in\mathbb{N}\right\}$$
 	Modifizierte Version: Wir brechen nach $n$ Versuchen ab, unabhängig davon ob ein Treffer erzielt wurde oder nicht.
 	$$\tilde{\Omega}=\left\{(x_1,...,x_n\}:x_i\in\{0,1\}\forall i=1,...,n\right\}$$
 	$X$ ist Anzahl der Versuche bis eine \textit{1} fällt. Dies wird dargestellt als Abbildung auf den Laplace-Raum $(\tilde{\Omega},\mathcal{P}(\tilde{\Omega}),P)$
 	\paragraph{Konkret:}
 	\[
 	X:(x_1,...,x_n)\in\tilde{\Omega}\rightarrow \left\{\begin{array}{lr}
 		k\in\{0,...,n-1\} &,\text{so dass } x=1 \text{ und } x_j=0\\
 		n &,\text{sonst} 	
 	\end{array}\right.
 	\]
 	$$\hookrightarrow\rho(k)=P_X\left(\{k\}\right)=\frac{|X^{-1}\left(\{k\}\right)|}{2^n}=
 	\frac{2^{n-k-1}}{2^n}=\left(\frac{1}{2}\right)^k *\frac{1}{2} \:\: \forall k=0,...,n-1$$
 	Das nennt man auch \textit{Poisson-Verteilung}
 	\subsection{Definition Poisson-Verteilung}
 	Sei $d>0$. Das W-Maß mit der Zähldichte 
 	$$\rho:\mathbb{N}_0\rightarrow[0,1],k \mapsto e^{-\lambda} \frac{\lambda^{k}}{k!}$$
 	heißt \textit{Poisson-Verteilung} zum Parameter $d$ und wird mit $P_{\lambda}$ bezeichnet.
 	\paragraph{Bemerkung:}
 	\begin{itemize}
 		\item $\lambda$ wird als mittlere Rate interpretiert, mit der ein Ereignis in einem vorgegebenen Zeitfenster $J$ beobachtet wird
 		\item $\mathcal{P}_{\lambda}$ ist die Verteilung der Zufallsvariablen \say{Anzahl der Ergebnisse in $J$} 		
 	\end{itemize}
 	\paragraph{Beispiele:}
 	\begin{itemize}
 		\item Anzahl der ankommenden E-Mails/Tag
 		\item Anzahl der Versicherungsfälle  pro Jahr
 		\item Anzahl der Kunden pro Stunde
 	\end{itemize}
 	\paragraph{Bemerkungen:}
 	$$\rho(k)=e^{-\lambda}\frac{\lambda^k}{k!}, k=0,1,...$$
 	$$\hookrightarrow \text{Dichte   } e^{\lambda}=\sum^{\infty}_{k=0}\frac{\lambda^{\lambda}}{k!}$$
	
	\section{Erwartungswerte und Varianz}
	\subsection{Definition Reelwertigen Zufallsvariable}
	Eine Zufallsvariable, die Werte in $\mathbb{R}$ annimmt heißt \textit{reele Zufallsvariable}. Alle bisherigen Zufallsvariablen sind Beispiele reeller Zufallsvariablen. Genauer handelt es sich bei den Zufallsvariablen um \textit{diskret verteilte} Zufallsvariablen. Das für diese ist, dass die Werte in einer höchstens abzählbaren Menge $\Omega\subset\mathbb{R}$ annehmen:
	
	\begin{itemize}
	\item $\Omega=\{0,1\} \text{ bei } X\sim \mathcal{B}_p \quad (p\in[0,1])$
	\item $\Omega=\{0,1,...,n\} \text{ bei } X\sim \mathcal{B}_{n,p}$
	\item $\Omega=\{0,1,...,m\} \text{ bei } X\sim\mathcal{H}_{m,k,N-k}$
	\item $\Omega=\mathbb{N} \text{ bei } X\sim \mathcal{P}_{\lambda} \text{ sowie } X\sim \mathcal{G}_p$ 
	\end{itemize}		
 	
 	Jede auf einem diskreten W-Raum $(\Omega',\mathcal{P}(\Omega'),P)$ definierte Zufallsvariable $X$ gehört zu den diskreten Zufallsvariablen, denn  	$X(\Omega')$ ist eine höchstens abzählbare Menge.
 	
 	\subsection{Definition Erwartungswert}
 	Sei $X$ eine reele Zufallsvariable, die Werte in einer endlichen Menge $\Omega\subset\mathbb{R}$ annimmt, dann heißt der Wert 
 	$$\mathbb{E}(X):=\sum_{x\in\Omega}xP(X=x)=\sum_{x\in\Omega}xP_X(\{x\})$$
 	\textit{Erwartungswert von $X$}. 
 	
 	\paragraph{}
 	Sei X eine Zufallsvariable, die abzählbar unendlich viele Werte $x_i,i\in\mathbb{N}$ annehmen kann, jeweils mit der Wahrscheinlichkeit $p$, so heißt
 	$$\mathbb{E}(X):=\sum^{\infty}_{i=1}x_i p_i$$
 	\textit{Erwartungswert} (EW) von X, wenn die eben aufgeführte Reihe absolut konvergent ist.
 	$$\hookrightarrow \sum^{\infty}_{i=1}x_i \text{ absolut konvergent } \Leftrightarrow \sum^{\infty}_{i=1}|x_i |<\infty$$
 	
 	Der Erwartungswert ist eine reelle Zahl.
 	\paragraph{Interpretation:}
 	\begin{itemize}
 	\item Diese Zahl gibt den Wert an, den die betreffende Zufallsvariable im Mittel annimmt. Dabei gewichten wir die einzelnen Werte $x$, die $X$ annehmen kann, entsprechend der Wahrscheinlichkeit $p_x$ des Eintretens des Elementarereignisses $X=x$.
	\item Wenn wir das Experiment $X$ unter identischen Bedingungen und ohne gegenseitige Beeinflussung oft wiederholen, erwarten wir, das dass arithmetische Mittel der einzelnen Ergebnisse für ein geeignetes W-Modell nah bei $\mathbb{E}(X)$ liegt
	\end{itemize}
 	
 	\paragraph{Beispiele}
 	\begin{itemize}
 	\item[a)] Erwartungswert von $X\sim\mathcal{B}_p$ \\ 
$\mathbb{E}(X)=0(1-p)+1*p=p$
 	\item[b)] $\Omega=\{x_1,...,x_n\}\subset\mathbb{R} \text{ und } X\sim\mathcal{U_{\Omega}}:$ \\
$\mathbb{E}(X) = \sum_{i=1}^n x_i \frac{1}{n} = \frac{1}{n} \sum^n_{i=1}x_i$ entspricht Mittel der Zahlen $x_1,...,x_n$
	\item[c) ] Erwartungswert von $X\sim\mathcal{B}_{n,p}$
	\begin{equation*}
	\begin{split}
	\mathbb{E}(X)=\sum^n_{k=0} k\mathcal{B}_{n,p}(\{k\}) & = \sum^n_{k=0}k\binom{n}{k}p^k (1-p)^{n-k} \\
	& = \sum^n_{k=1}k\frac{n!}{(n-k)!k!}p\,p^{k-1}(1-p)^{n-k}\\
	& = \sum^n_{k=1}np\binom{n-1}{k-n}p^{k-1}(1-p)^{n-k}\\
	& = np\sum^{n-1}_{l=o}\binom{n-1}{l}p^l (1-p)^{n-1-l}=np
	\end{split}
	\end{equation*}	
	\item[d) ] Erwartungswert von $X\sim\mathcal{P}_{\lambda}$:
	\begin{equation*}
	\begin{split}
	\mathbb{E}(X) & = \sum^{\infty}_{k=0}k\,\mathcal{P}_{\lambda}(\{k\})\\
	 & = \sum^{\infty}_{k=1}ke^{-\lambda}\frac{\lambda^k}{k!}\\
	 & = e^{-\lambda}\sum^{\infty}_{k=1}\frac{\lambda^{k-1}}{(k-1)!}\\
	 & =\lambda
	\end{split}
	\end{equation*}
 	\end{itemize}
 	
 	\paragraph{Zusammenhang zwischen Poisson-und  Binomialverteilung}
 	\paragraph{Beispiel:}
 	\begin{itemize}
 	\item Sei $\lambda>0$ die mittlere Anzahl von Kunden, die ein Blumengeschäft im Zeitintervall $I$ (z.B. $I=$ 10 Stunden) betreffen.
 	\item Wir zerlegen das Zeitintervall $I$ in $n$ gleich große Teilintervalle
 	\item Mit wachsendem $n$ wird bei festen $I$ die Länge der Teilintervalle\footnote{$=\frac{I}{n}$} immer kürzer
 	\item[$\hookrightarrow$] Ansatz für die Wahrscheinlichkeit $p_n$, dass ein Teilintervall der Länge $\frac{I}{n}$ ein Kunde den Laden betritt
 	$$p_n=\frac{\lambda}{n}$$
 	\end{itemize}
 	
 	\paragraph{Annahme:}
 	Ob ein Kunde im gegebenen Teilintervall den Laden betritt ist unabhängig davon, ob im anderen Teilintervall das Ereignis eintritt.\\
 	$\hookrightarrow$ Die Wahrscheinlichkeit dafür, dass im Intervall $I$ der Blumenladen von $k$ Kunden besucht wird, wird entsprechend durch die Binomialverteilung bestimmt:
 	$$\mathcal{B}_{n,\, p_n=\frac{\lambda}{n}}(\{k\})=\binom{n}{k}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k}$$
 	Dann lässt sich zeigen:
 	$$\lim_{n\to\infty}\mathcal{B}_{n,\frac{\lambda}{n}}(\{k\})=\mathcal{P}_{\lambda}(\{k\})$$
 	
 	\subsection{Theorem}
 	Seien $\lambda>0$ und $p_n\in(0,1),\, n\in\mathbb{N}$ eine Folge, so dass $\lim_{n\to\infty}n\, p_n=\lambda$. Dann gilt für jedes $k\in\mathbb{N}$:
 	$$\lim_{n\to\infty}\mathcal{B}_{n,p_n}(\{k\})=\mathcal{P}_{\lambda}(\{k\})$$
 	\subsection{Definition}
 	Seien $f:\mathbb{N}\to\mathbb{R}$ und $g:\mathbb{N}\to\mathbb{R}$ zwei Folgen 
 	$$f(k)\sim g(k) \text{ für } k\to\infty \Leftrightarrow \frac{f(k)}{g(k)}=1 \text{ für } k\to\infty$$
 	
 	\subsection{Lemma}
 	Sei $k\in\mathbb{N}$, dann gilt im Grenzwert $n\to\infty$
 	$$\binom{n}{k}\sim \frac{n^k}{k!}$$
 	
 	\paragraph{Beweis zu Lemma 5.4}
 	\begin{equation*}
 	\begin{split}
 	\binom{n}{k} = \frac{n!}{(n-k)!k!} & = \frac{n(n-1)...(n-k+1)}{k!}\\
 	& = \frac{n^k}{k!}\frac{(n-1)(n-2)...(n-k+1)}{n^{k-1}}\\
 	& = \frac{n^k}{k!}\frac{n\left(1-\frac{1}{n}\right)}{n} \frac{n\left(1-\frac{2}{n}\right)}{n} ... \frac{n\left(1-\frac{k+1}{n}\right)}{n}\\
 	& \sim \frac{n^k}{k!}
 	\end{split}
 	\end{equation*}
 	
 	\subsection{Theorem}
 	Sei $\Omega',\mathcal{P}(\Omega'),P)$ eine diskreter W-Raum und $X,Y$ zwei Zufallsvariablen mit $\mathbb{E}(X)$ und $\mathbb{E}(Y)$ definiert. Dann gilt:
 	\begin{itemize}
 	\item[a)] $\mathbb{E}(cX) = c\mathbb{E}(X) \quad \forall c\in\mathbb{R}$
 	\item[b)] $\mathbb{E}(X+Y) = \mathbb{E}(X) + \mathbb{E}(Y)$
 	\item[c)] $X\leq Y \Rightarrow\mathbb{E}(X)\leq\mathbb{E}(Y)$
 	\end{itemize}
 	\paragraph{Beweis:}
 	a)
 	$$\mathbb{E}(cX)\stackrel{1}{=}\sum_{z\in\Omega}zP_{cX}(\{z\})\,
 	=\sum_{z\in c\Omega}zP_X\left(\left\{\frac{z}{c}\right\}\right)
 	=\sum_{x\in\Omega}cxP_X(\{x\})=c\mathbb{E}(X)$$
 	\begin{enumerate}
 	\item cX ist eine Zufallsvariable mit Werten in $c\Omega\,=:\{cx\in\mathbb{R}:\,x\in\Omega\}$ eine Verteilung $P_cX$
 	\item 
 	\begin{equation*}
 	\begin{split}
 	cX:\, & \Omega\mapsto c\Omega \quad \text{ ist eine Zufallsvariable über }\\ 
 	& x\mapsto cX \quad (\Omega,\mathcal{P}(\Omega),P_X) \text{ , wobei}
 	\end{split}
 	\end{equation*}
	$$P_X=P\circ X^{-1}$$
	$$\hookrightarrow P_{cX}=P_X\circ (cX)^{-1} \Rightarrow P_cX(\{z\})=P_X(cX=z)
	=P_X\left(\left\{\frac{z}{c}\right\}\right)$$	 	
 	\end{enumerate}
 	
 	\subsection{Definition Varianz}
 	\subsubsection{Endliche Zufallsvariable}
 	Sei $X$ eine endlich Zufallsvariable, mit Werten $x\in\Omega$, mit Wahrscheinlichkeit $p_x$. Dann heißt:
 	$$\sum_{x\in\Omega}\left(x-\mathbb{E}\left(X\right)\right)^2\,p_x=:\text{Var}(X)$$
 	die \textit{Varianz}\footnote{Auch \textit{Streuung} genannt.} von $X$. 
 	\subsubsection{Abzählbare Zufallsvariable}
 	Sei $X$ eine abzählbar, unendliche Zufallsvariable, mit den Werten $x\in\Omega,\,i\in\mathbb{N}$, mit einem abzählbaren $\Omega$  mit Wahrscheinlichkeit $p_i$ und der Erwartungswert $\mathbb{E}(X)$ ist konvergent. Dann heißt:
 	$$\sum_{i=1}^{\infty}\left(x_i -\mathbb{E}\left(X\right)\right)^2 p_i =:\text{Var}(X)$$
 	die Varianz von $X$.
 	\paragraph{Bemerkung:}
 	\begin{equation*}
 	\begin{split}
 	\text{Var}(X) & \geq 0\\
 	\text{Var}(X) & = \infty \text{ sind möglich}
 	\end{split}
 	\end{equation*}
 	$$\forall X\exists v\in\mathbb{R}\cup\{+\infty\}\quad \text{Var}(X)=v$$
 	\subsection{Definition Standardabweichung}
 	Sei $X$ eine Zufallsvariable mit $\text{Var}(X)\in\mathbb{R}$, dann wird
 	$$\sigma(X)=\sqrt{\text{Var}(X)}$$
 	\textit{Standardabweichung} genannt.
 	\paragraph{Interpretation:}
 	Die Varianz gibt den Mittelwert der Abweichung zum Quadrat der Zufallsvariable $X$ gegenüber des Erwartungswertes $\mathbb{E}(X)$ an.
 	\begin{itemize}
 	\item[a)] $X\sim\mathcal{B}_p$, mit $\mathbb{E}(X)=p$
 	\begin{equation}
 	\begin{split}
 	\text{Var}(X) & = \left(1-p\right)^2 p+\left(0-p\right)^2\left(1-p\right)\\
 	& = \left(1-p\right)\left(p\left(1-p\right)+p^2\right)\\
 	& = \left(1-p\right)\left(p-p^2 + p^2\right)\\
 	& = \left(1-p\right)p\\
 	\text{und } \sigma(X)& = \sqrt{\left(1-p\right)p}
 	\end{split}
 	\end{equation}
 	
 	\item[b)] $X\sim\mathcal{U}_{\Omega_n}$ mit $\Omega_n = \{x_1,...,x_n\},\quad x_i\in\mathbb{R}$
 	\begin{equation*}
 	\begin{split}
 	\text{Dann ist } \mathbb{E}(X)& =\frac{1}{n}\sum_{x\in\Omega_n}x=:\bar{x}\\
 	\hookrightarrow\text{Var}(X)& = \sum_{i=1}^n\left(x_i-\bar{x}\right)^2\frac{1}{n}\\
 	& = \frac{1}{n}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\\
 	& = \frac{1}{n}\sum_{i=1}^n\left(x_i^2-2x_i\bar{x}+\bar{x}^2\right)\\
 	& = \frac{1}{n}\sum_{i=1}^n x^2_i -\frac{2\bar{x}}{n}\sum_{i=1}^n x_i +\bar{x}^2\\
 	& = \frac{1}{n}\sum_{i=1}^n x_i^2 -\bar{x}^2\\
 	& = \mathbb{E}(X^2)-\mathbb{E}(X)^2
 	\end{split}
 	\end{equation*}
 	\end{itemize}
 	
 	\subsection{Definition Zweiter Moment}
 	Sei $X$ eine Zufallsvariable mit Werten $x\in\Omega$ mit Wahrscheinlichkeiten $p_x$, wenn 
 	$$\sum_{x\in\Omega}x^2p_x$$ 
 	konvergent, dann nennt man diesen Wert \textit{das zweite Moment von $X$}.
 	$$\mathbb{E}(X^2)=\sum_{x\in\Omega}x^2p_x$$
 	\subsection{Theorem}
 	Sei $X$ eine Zufallsvariable mit reellen, diskreten Werten und dem Erwartungswert $\mathbb{E}(X)=m$. Wenn $\mathbb{E}(X^2)\in\mathbb{R}$, dann gilt:
 	$$\text{Var}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$$.
 	
 	\subsubsection{Beispiele}
 	\paragraph{Beispiel 1:}
 	$X\sim \mathcal{B}_{n,p}$ , $\mathbb{E}(X)=np$ , Var$(X)=$ ? bzw. $\sigma(X)=$?
	\begin{equation*}
	\begin{split}
	\text{Var}(X) & = \mathbb{E}(X^2)-(np)^2\\
	& = \sum_{k=o}^n k^2 \binom{n}{k}p^k (1-p)^{n-k} -(np)^2\\
	& = \sum_{k=1}^n npk\binom{n-1}{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)}\\
	& = np\sum_{l=0}^{n-1} (l+1)\binom{n-1}{l}p^l(1-p)^{(n-1)-l} , \quad 
\tilde{X}\sim\mathcal{B}_{n-1,p}\\
	& = np\left(\mathbb{E}(\tilde{X})+1\right)-(np)^2 = np\left(\left(n-1\right)p+1\right)-(np)^2
= np(1-p)
	\end{split}
	\end{equation*}	 	
 	und $\sigma(X)=\sqrt{\text{Var}(X)}=\sqrt{np(1-p)}$
 	
 	\paragraph{Beispiel 2}
 	$X$ ist eine Zufallsvariable mit Werten in $\{-1,1\}$ und zugehöriger Zähldichte
 	$$\rho(-1) = \frac{1}{2} \text{ und } \rho(1)=\frac{1}{2}$$
 	\begin{equation*}
 	\begin{split}
 	\hookrightarrow \mathbb{E}(X) & = (-1)\frac{1}{2} + 1\cdot \frac{1}{2}=0\\
 	\text{Var}(X) & = (-1)^2\frac{1}{2} + 1^2\cdot \frac{1}{2}=\frac{1}{2} + \frac{1}{2}=1\\
 	\end{split}
 	\end{equation*}
 	$$\hookrightarrow \sigma(X)=\sqrt{1}=1$$
 	
 	\paragraph{Beispiel 3}
 	$$X\sim P_{\lambda} \quad (\lambda>0), \: \mathbb{E}(X)=\lambda$$
 	Wir brauchen das zweite Moment von $X$:
 	\begin{equation*}
 	\begin{split}
	\mathbb{E}(X^2) & =\sum_{k=0}^{\infty}k^2 e^{-\lambda}\frac{\lambda^k}{k!} \\
	& = \lambda\cdot\sum_{k=1}^{\infty}k\cdot e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\\
	& = \lambda\cdot\sum_{l=0}^{\infty}(l+1)e^{-l}\frac{\lambda^l}{l!} \: , \, l:=k-1\\
	& = \lambda\cdot (\lambda+1)=\lambda^2 + \lambda	
 	\end{split}
 	\end{equation*}
 	
 	\section{Stochastische Unabhängigkeit}
	\subsection{Definition}
	Sei $\left(\Omega,\mathcal{P}(\Omega),P\right)$ ein diskreter W-Raum. Zwei Ereignisse $E,F\in\mathcal{P}(\Omega)$ heißen \textit{stochastisch Unabhängig} bezüglich $P$, wenn
	$$P(E\cap F)=P(E) \cdot P(F)$$
	gilt.
	\paragraph{Beispiel:}
	Es wird ein blauer und ein roter Spielwürfel geworfen. Im entsprechenden Laplace-Modell mit dem Ergebnisraum $\Omega=\{1,...,6\}^2$ sind die Ereignisse:
	\begin{itemize}
	\item[a)] $B$: der blaue Würfel zeigt die Augenzahl ``2''\\
	$R$: der rote Würfel zeigt die Augenzahl ``4''\\	
	stochastisch unabhängig, denn
	$$|B|=|\left\{\left(b,r\right)\in\Omega: \: b=2\right\}|=6 \Rightarrow\: P(B)=\frac{6}{36}=\frac{1}{6}$$
	$$|R|=|\left(\{\left(b,r\right)\in\Omega: \: r=4\right\}|=6 \Rightarrow\: P(R)=\frac{1}{6}$$
	$$|B\cap R|=|\{(2,4)\}=1 \Rightarrow P(B\cap R) =\frac{1}{36}$$
	d.h. $P(B\cap R) = P(B)\cdot P(R)$

	\item[b)] $B$ und $R_{>2}$: der rote Würfel zeigt die Augenzahl $>2$ stochastisch unabhängig, denn 
	$$|R_{>2}| = |\{(b,r)\in\Omega: \: r>2\}| = 6\cdot 4= 24 \Rightarrow P(R_{>2})=\frac{2}{3}$$
	$$|B\cap R_{>2})=\frac{1}{9} = \frac{2}{3} \cdot \frac{1}{6} = P(B) \cdot P(R_{>2})$$

	\item[c)] $B$ und $E_g$: beide Augenzahlen sind gerade \\
	stochastisch \textit{abhängig}, denn
	$$|E_g|=|\{(b,r)\in\Omega : \: b,r\in\{2,4,6\}\}|=9 \Rightarrow\: P(E_g)=\frac{1}{4}$$
	$$|E_g \cap B|=|\{(b,r)\in\Omega : \: b=2,r\in\{2,4,6\}\}|=3 $$
	$$\Rightarrow P(E_g \cap B)= \frac{1}{12}\neq \frac{1}{4} \cdot \frac{1}{6}=P(E_g)\cdot P(B)$$ 
	
	\item[d)] $B$ und $E$: beide Augenzahlen sind gerade oder beide sind ungerade
	$$E=E_g \cup E_u \quad \Rightarrow P(E)=\frac{18}{36}=\frac{1}{2}$$
	so dass $B$ und $E$ stochastisch unabhängig sind bezüglich $P=\mathcal{U}_{\Omega}$
	\end{itemize}
	
	\paragraph{Bemerkung:}
	Sei $\left(\Omega,\mathcal{P}\left(\Omega\right)\right)$ ein diskreter Ereignisraum. Dann sind je zwei disjunkte Ereignisse $E,F\in\mathcal{P}(\Omega)$ mit $E,F\neq\varnothing$ stochastisch abhängig bezüglich jedem W-Maß auf $\mathcal{P}(\Omega)$, dessen zugehörige Zähldichte $\rho$ auf $\Omega$ strikt positiv ist $\left(\rho\left(\omega\right)>0\, \forall \omega\in\Omega\right)$, denn 
	$$P(E\cap F)=P(\varnothing)=0\neq \underbrace{P(E)}_{=\sum_{\omega\in E}\rho(\omega)>0}\cdot P(F)>0$$
	
	\subsection{Definition Unabhängige Zufallsvariablen}
	\label{sec6.2}
	Seien $(\Omega,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum und $X,Y$ zwei Zufallsvariablen auf $(\Omega,\mathcal{P}(\Omega),P)$ jeweils mit Werten in $\Omega_X$ beziehungsweise $\Omega_Y$. Dann heißen $X$ und $Y$ unabhängige Zufallsvariablen, wenn $\left\{X\in E\right\}$ und $\left\{Y\in F\right\}$ bezüglich $P$ stochastisch unabhängig sind für jede Wahl von Ereignissen $E\subset\Omega_X$ und $F\subset\Omega_Y$.
	\paragraph{Beachte:}
	$$\left\{X\in E\right\}=X^{-1}\underbrace{\left(E\right)}_{\in\Omega_X}=\{\omega\in\Omega:\: X(\omega)\in E\}$$  

	Wir können die Unabhängigkeit von $X$ \& $Y$ wie folgt definieren(*):\\
	$X,Y$ sind unabhängige Zufallsvariablen 
	$$:\Leftrightarrow P(X\in E, Y\in F) = P(X\in E) \cdot P(Y\in F) \quad 	\forall E\subseteq\Omega_X \: , \forall F\subseteq\Omega_Y$$
	Aus dieser Relation folgt(**):
	$$P(X=x,Y=y)=P(X=x)\cdot P(Y=y) \quad \forall x\in\Omega_X \text{ und } \forall y\in\Omega_Y $$
	Umgekehrt impliziert (**) die Relation (*), d.h. das $X$ und $Y$ unabhängig sind. Wie sieht man das?
	\begin{itemize}
	\item $x\in\Omega\longmapsto\rho_X (x):=P(X=x)$ ist die Gewichtsfunktion der Verteilung  $P_X$ von $X$
	\item $y\in\Omega_Y\longmapsto\rho_Y(y):=P(Y=y)$ ist die Gewichtsfunktion der Verteilung $P_Y$ von $Y$
	\item $(x,y)\in\Omega_X \times \Omega_Y\longmapsto P(X=x,Y=y)$ ist die Gewichtsfunktion der Verteilung einer Zufallsvariable mit Werten aus $\Omega_X \times \Omega_Y$
	\end{itemize}
	\subsection{Lemma Unabhängigkeit}
	Unter der Voraussetzung aus der Definition Unabhängige Zufallsvariablen (siehe 6.2) sind zwei Zufallsvariablen $X$ und $Y$ auf $(\Omega,\mathcal{P}(\Omega),P)$ unabhängig genau dann, wenn für jedes $x\in\Omega_X$ und jedes $y\in\Omega_Y$ gilt
	$$P(X=x,Y=y)=P(X=x)\cdot P(Y=y)$$
	\subsubsection{Beispiel 1}
	$\mathcal{E}$: n-maliges Werfen eines Spielwürfels\\
	$\hookrightarrow$ Der W-Raum zu diesem Zufallsexperiment ist:
	$$(\Omega,\mathcal{P}(\Omega),P=\mathcal{U}_{\Omega}) \text{ mit } \Omega:=\{1,...6\}^n$$
	Seien $1\leq k , l\leq n$ mit $k\neq l$. Dann sind
	\begin{itemize}
	\item $X_k$: Augenzahl bei Wurf k, und
	\item $X_l$: Augenzahl bei Wurf l
	\end{itemize}			 	
 	Denn für jedes $x\in\Omega_{X_k}:=X_k(\Omega)$ und jedes $y\in\Omega_{X_l}:=X_l(\Omega)$ erhalten wir
 
 	\begin{equation*}
	\begin{split}
	P(X_k =x,X_l = y) & = P(\{\omega\in\Omega:\,\omega_k =x ,\omega_l = y\})\\
	& = \frac{6^{n-2}}{6^n}=\frac{1}{6}\cdot\frac{1}{6}=P(X_k=x)\cdot P(X_l =y)
	\end{split}
	\end{equation*}	 	
 	Nach Lemma 6.3 sind somit $X_k$ und $X_l$ zwei unabhängige Variablen.
 	\subsubsection{Beispiel 2}
 	\paragraph{Gegeben:}  eine Urne mit 10 gleichartigen, aber nummerierten Kugeln.
 	\paragraph{Zufallsexperiment:} Stichprobe ohne Zurücklegen: es werden 2 Kugeln nacheinander gezogen
 	\begin{itemize}
 	\item $X$: Nummer der ersten gezogenen Kugel
 	\item $Y$: Nummer der zweiten gezogenen Kugel
 	\end{itemize}
 	\subsection{Definition Familie von Ereignissen}
 	Seien $(\Omega ,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum und $I$ eine beliebige, nicht leere Indexmenge. Eine Familie von Ereignissen $E_i\in\mathcal{P}(\Omega)\in I$, heißt \textit{stochastisch unabhängig}, wenn für jede endliche Teilmenge $J\in I$
 	$$P\left(\bigcap_{j\in J}E_j\right)=\prod_{j\in J}P(E_j)$$
 	gilt.
 	\paragraph{Beispiel:}
 	(Kontext: Beispiele zur Definition 6.1)\\
 	Die Familie der Ereignisse $B,E$ und $R$ ist nicht stochastisch unabhängig, obwohl die Ereignisse \textit{paarweise} stochastisch unabhängig sind ( Wie wir bereits nachgerechnet haben), denn
 	$$B\cap\ R\cap E=\varnothing \quad \Rightarrow P(\varnothing )=0\neq P(E)\cdot P(R)\cdot P(B)$$
 	\paragraph{Bemerkung:}
 	Auch die Familie $B,E$ und $R_4$: \textbf{roter Würfel zeigt ``4''} ist stochastisch \textit{nicht} unabhängig.
 	
 	\subsection{Definition Unabhängige Familie}
 	Seien $(\Omega,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum, $I$ eine beliebige, nicht-leere Indexmenge und $X_i ,i\in I$ eine Familie von Zufallsvariablen auf $(\Omega,\mathcal{P}(\Omega),P)$ jeweils mit Werten in einer höchstens abzählbaren
 	$\Omega_{X_i}$. Dann heißt die Familie $X_i ,i\in I$ unabhängig, wenn für jede Wahl von Ereignissen $E_i \subset\Omega_i , i\in I$ eine stochastisch unabhängige Familie ist.
 	\paragraph{Standardbeispiel:}
 	Seien $n\in\mathbb{N}$ , $\Omega=\{1,...,6\}^n$ und  	
 	$$\underbrace{(\Omega,\mathcal{P}(\Omega),P=\mathcal{U}_{\Omega})}_{\text{Experiment } \mathcal{E}: \text{ n-maliges würfeln eines Würfels}} $$. \\
 	Wir betrachten:
 	$$X_i : (\omega_1,...\omega_n)\in\Omega\longmapsto\omega_i$$
 	$$P(X_{i_1}=x_{i_1},...,X_{i_m}=x_{i_m})=\frac{6^{n-m}}{6^n}=\frac{1}{6^m}=
 	\prod^m_{l=1}P(X_{i_l}=x_{i_l})$$
 	\section{Bedingte Wahrscheinlichkeit}
 	\paragraph{Formaler Rahmen:}
 	$(\Omega,\mathcal{P}(\Omega),P)$ diskretes W-Raum
 	\begin{itemize}
 	\item[] \emph{Gegeben}: $A,B\subseteq\Omega$ zwei Ereignisse
 	\item[] \emph{Annahme}: $P(A)>0$
 	\end{itemize}
 	 	
 	
 	\paragraph{Szenario:}
 	Wir beobachten, dass $A$ eingetreten ist.\\
 	$\hookrightarrow$ Frage: Wie ändert sich unter der obigen Annahme unsere Wahrscheinlichkeitsbewertung für $B$?\\
 	$\hookrightarrow$ Fallunterscheidung:
 	\begin{itemize}
 	\item Sind $A$ und $B$ stochastisch unabhängig\\
 	$\hookrightarrow$ kein Anlass für eine Neubewertung
 	\item $A$ \& $B$ sind nicht stochastisch unabhängig\\
 	$\hookrightarrow$ Wie genau soll die Neubewertung ausfallen?
 	\end{itemize}
 	\paragraph{Beispiel:}
 	Gegeben ist eine Urne mit $N$ Kugeln, davon genau $s$ schwarze und $w$ weiße. D.h. $N=s+w$.
 	$\hookrightarrow$ Modell: $U=\underbrace{\{1,...,w\}}_{\text{Teilmenge der weißen Kugeln}} \cup\underbrace{\{ w+1,...,s+w\}}_{\text{Teilmenge der schwarzen Kugeln}}$
 	\paragraph{Zufallsexperiment:}
 	Es werden zwei Kugeln gezogen, ohne Zurücklegen.
 	$$\hookrightarrow \text{ Ergebnismenge }: \Omega =\{(k,l):\: k,l\in U, k\neq l\}$$
 	$$\hookrightarrow |\Omega | = N\cdot (N-1) = (s+w)\cdot (s+w-1)$$
 	$$P=\mathcal{U}_{\Omega} \text{ Gleichverteilt auf } \Omega$$
 	Wir betrachten die Ereignisse:
 	\begin{itemize}
 	\item[A]: 1. gezogene Kugel ist weiß
 	\item[B]: 2. gezogene Kugel ist weiß
 	\end{itemize}
 	
 	A und B sind \emph{nicht} stochastisch unabhängig bezüglich $P=\mathcal{U}_{\Omega}$, wobei $\Omega=\{(k,l):\: k,l\in\mathcal{U},k\neq l\}$, denn
 	\begin{itemize}
 	\item $P(A)=\frac{|A|}{(w+s)(w+s-1)}=\frac{w(w+s-1)}{(w+s)(w+s-1)}=\frac{w}{w+s}$
 	\item $P(B)=P(A)$ , da $|A|=|B| \:$ ($f:A\rightarrow B,(k,l)\rightarrow(l,k)$ ist eine Bijektion)
 	\item $P(A\cap B)=\frac{|A\cap B|}{(w+s)(w+s-1)}\frac{w\cdot (w-1)}{(w+s)(w+s-1)}\neq \left(\frac{w}{w+s}\right)^2=P(A)\cdot P(B)$\\
 	$\hookrightarrow$ Bedingung für stochastische Unabhängigkeit \emph{nicht} erfüllt $\Rightarrow$ A \& B stochastisch unabhängig bezüglich $P=\mathcal{U}_{\Omega}$
 	\end{itemize}
 	\paragraph{Annahme:}
 	A ist eingetreten (1. gezogene Kugel ist weiß)
 	\paragraph{$\hookrightarrow$ Frage:} 
 	Wie groß ist die Wahrscheinlichkeit für B unter dieser Annahme?
 	\paragraph{Antwort:}
 	$$\frac{w-1}{(w+s-1)}\neq\frac{w}{s+w}=P(B)$$
 	Hier gilt: 
 	$$\frac{w-1}{w+s-1}=\frac{P(A\cap B)}{P(A)}=\frac{w(w-1)}{(w+s)(w+s-1)}\cdot\frac{w+s}{w}$$
 	\subsection{Definition Bedingte Wahrscheinlichkeit}
 	Sei $(\Omega,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum und $A\subseteq\Omega$ mit $P(A)>0$. Dann heißt für jedes $B\subseteq\Omega$
 	$$P(B|A):=\frac{P(A\cap B)}{P(A)}$$
 	\textit{bedingte Wahrscheinlichkeit} für $B$ gegen $A$.
 	\paragraph{Beachte:}
 	Im Spezialfall, das $A$ \& $B$ stochastisch unabhängig bezüglich $P$ sind, gilt
 	$$P(B|A)=P(B) \: \left(\text{denn } P(B|A)=\frac{P(A\cap B)}{P(A)}=\frac{P(A)\cdot P(B)}{P(A)}\right)$$
 	\paragraph{Bemerkung:}
 	Bedingen macht keine Aussagen über Kausalitäten!\\
 	$\hookrightarrow$ Beispiel:
 	\begin{itemize}
 	\item 1. gezogene Kugel bleibt verdeckt
 	\item 2. gezogene Kugel ist weiß (d.h. $B$ tritt ein)
 	\item[$\hookrightarrow$] \textbf{Frage:} Wahrscheinlichkeit dafür, dass $A$ eingetreten ist, d.h. die verdeckte Kugel weiß ist?
 	\item[$\hookrightarrow$] \textbf{Antwort:} $P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{w-1}{w+s-1}\neq P(A)$
 	\end{itemize}
 	$\hookrightarrow$ Das Ereignis $B$ hat keinen Einfluss auf das Eintreten von $A$. Trotzdem bewirkt $B$ eine Neubewertung der Wahrscheinlichkeit für $A$. Das liegt daran das $B$ Informationen aus $A$ enthält. $B$ wird nämlich durch $A$ beeinflusst.
	
	\paragraph{Beispiel:}
	\label{defBedingteWS}
	Ziehen einer Kugel aus einer von 2 Urnen.
	\begin{itemize}
	\item[Urne 1:] enthält 7 rote und 3 schwarze Kugeln
	\item[Urne 2:] enthält 9 rote und 1 schwarze Kugel
	\end{itemize}	 	
 	Es wird zufällig entschieden aus welcher Urne gezogen wird. (z.B. auf der Grundlage des Ergebnisses eines Münzwurfes)
 	\paragraph{Ergebnismenge des Zufallsexperimentes:}
 	$$\Omega=\{1,2\}\times\{1,...,10\}=\{(a,b):\: a\in\{1,2\} \: , \: b\in\{1,...,10\}\}$$
 	Wir betrachten die Ereignisse:
 	\begin{itemize}
 	\item[$A$] $=\{1\}\times\{1,...,10\}=\{(a,b)\in\Omega:\: a=1\}$ (es wird aus Urne 1 gezogen)
 	\item[$B$] $=\{2\}\times\{1,...,10\}=\{(a,b)\in\Omega:\: a=2\}$ (es wird aus Urne 2 gezogen)
 	\item[$R$] $=\{(a,b)\in\Omega\: ,\: a=1,\: b\leq 7\}\cup\{(a,b)\in\Omega:\: a=2,\: b\leq 9\}$
 	\item[$S$] $=\{(a,b)\in\Omega\: ,\: a=1,\: b>7 \}\cup\{(a,b)\in\Omega:\: a=2,\: b=10\}$ 
 	\end{itemize}
 	\paragraph{$\hookrightarrow$ Frage:}
 	\begin{itemize}
 	\item Wahrscheinlichkeit, dass eine rote Kugel gezogen wird, wenn die Wahl für Urne 1 fällt. D.h. wir fragen nach der bedingten Wahrscheinlichkeit:
 	$$P(R|A)=\frac{P(R\cap A)}{P(A)}$$
 	\item Wahrscheinlichkeit dafür, dass eine schwarze Kugel gezogen wird, wenn die Wahl auf Urne 1 fällt:
 	\begin{equation*}
 	\begin{split}
 	P(S|A) & =\frac{P(S\cap A)}{P(A)}\\
 	& = \frac{|S\cap A|}{|\Omega|}\cdot\frac{|\Omega|}{|A|}\\
 	& = \frac{|S\cap A|}{|A|}\\
 	& =\frac{3}{10}
 	\end{split}
 	\end{equation*}
 	\item Wie groß ist die Wahrscheinlichkeit, eine rote Kugel zu ziehen? D.h. $P(R)=$?
 	$$P(R)=\underbrace{\frac{1}{2}\cdot\frac{7}{10}+\frac{1}{2}\cdot\frac{9}{10}}_{\text{Wofür stehen diese Zahlen genau}}$$
 	\begin{equation*}
 	\begin{split}
	P(R) & =P(A)\cdot P(R|A) + P(B)\cdot P(R|B)\\
	& =P(A)\cdot\frac{P(R\cap A)}{P(A)}+P(B)\cdot\frac{P(R\cap B)}{P(B)}\\
	& (\text{da gilt: } A\cap B = \varnothing \Rightarrow (A\cap R) \text { und } (B\cap R) \text{ sind disjunkt})\\
	& =P\left(\left(R\cap A\right) \cup \left(R\cap B\right)\right)\\
	& =P\left(R\cap\underbrace{(A\cup B)}_{=\Omega}\right)\\
	& =P(R)	
 	\end{split}
 	\end{equation*}
 	\end{itemize}
 	
 	\subsection{Satz von der totalen Wahrscheinlichkeit}
 	Sei $(\Omega,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum und $n\in\mathbb{N}$. die Ereignisse $H_1,...H_n \subset\Omega$ seien eine Zerlegung von $\Omega$, d.h. 
 	$$H_i \cap H_j = \varnothing \quad \forall i,j\in\{1,...,n\},\: i\neq j \text{ und } \bigcup_{i=1}^n H_i =\Omega$$
 	Gilt $P(H_j)>0$  $\forall j=1,...,n$ , dann gilt für jedes Ereignis $A\subset\Omega$
 	$$P(A)=\sum_{j=1}^n P(H_j)\cdot P(A|H_j)$$
 	Eine weitere mögliche Frage: Wie groß ist die Wahrscheinlichkeit, dass die gezogene rote Kugel aus Urne 1 stammt.(siehe \ref{defBedingteWS})
 	\begin{itemize}
 	\item[$\hookrightarrow$] Frage: $P(A|R)=?$
 	\item[$\hookrightarrow$] \say{Problem}: Aus der Aufgabenstellung ist statt $P(A|R)$ die bedingte Wahrscheinlichkeit $P(R|A)$ leicht herzuleiten 
 	\item[$\hookrightarrow$] Ansatz siehe \say{Formel von Bayer}(siehe  \ref{satzBayer})	
 	\end{itemize}
 	
 	\subsection{Satz Formel von Bayer}
 	\label{satzBayer}
 	Für alle Ereignisse $A,B\subseteq\Omega$ mit $P(A)>0$ gilt
 	$$P(B|A)=\frac{P(A|B)\cdot P(B)}{P(A)}$$
 	\paragraph{Beweis:}
 	\begin{equation*}
 	\begin{split}
	\frac{P(A|B)\cdot P(B)}{P(A)} & =\frac{\frac{P(A\cap B)}{P(B)}\cdot P(B)}{P(A)}\\
	& =\frac{P(A\cap B)}{P(A)}\\
	& =P(B|A) 	
 	\end{split}
 	\end{equation*}
 	
 	\paragraph{Beispiel:}
 	Ziehen einer Kugel aus einer von zwei Urnen:
 	\begin{itemize}
 		\item[] Urne 1: genau 7 rote Kugeln und 3 schwarze Kugeln $\Rightarrow P(R|A)=\frac{7}{10}$
 		\item[] Urne 2: genau 1 rote und 9 schwarze Kugeln $\Rightarrow P(R|B)=\frac{1}{10}$
  	\end{itemize}
 	enthält.
 	$$\hookrightarrow P(R)=P(R|A)\cdot P(A) + P(R|B)\cdot P(B)=\frac{2}{5}$$
 	\paragraph{$\hookrightarrow$ Frage:}
	$P(A|R)= ?$ ( $\widehat{=}$ Wahrscheinlichkeit dafür, dass die gezogene Kugel aus Urne 1 stammt, wenn die Kugel rot ist.) 
	\paragraph{Antwort:}
	$$P(A|R)=\frac{P(R|A)\cdot P(A)}{P(R)}=\frac{\frac{7}{10}\cdot 
\frac{1}{2}}{\frac{2}{5}}=\frac{7}{8}$$	
 	\paragraph{Zusammenfassung:}
 	Durch das Eintreten eines Ereignisses A muss die Wahrscheinlichkeit von allen anderen Ereignissen angepasst werden.\\
 	$\hookrightarrow$ Übergang zu einem neuen/modifizierten W-Maß $P_A$, welches die Information über das Eintreten von $A$ berücksichtigt.\footnote{$P\rightsquigarrow^A P_A$}\\
 	Wir fordern:
 	\begin{itemize}
 		\item[F1)] $P_A(A)=1$ , \textbf{Motivation:} A stellt ein sicheres Ereignis dar
 		\item[F2)] Es gibt ein $k_A >0$, so dass für jedes $C\subset A$gilt:
 		$$P_A (C)=k_A \cdot P(C)$$
 		$\hookrightarrow$ \textbf{Motivation/Beispiel:}
 		\[
 		\text{WS für }=\left\{\begin{array}{lr}
 		\text{Schnee} & = \frac{1}{3}\\
 		\text{Regen} & = \frac{1}{3}\\
 		\text{niederschlagsfrei} & = \frac{1}{3}
 	\end{array}\right.
 	\] 
 	\end{itemize}
 	
 	\subsection{Satz Existenz und Eindeutigkeit bedingter W-Maße}
 	Sei $(\Omega,\mathcal{P}(\Omega),P)$ ein diskreter W-Raum und $A\subset\Omega$ mit $P(A)>0$. Dann existiert genau ein W-Maß
 	$$P_A\: : \: \mathcal{P}(\Omega)\rightarrow [0,1]$$
 	mit den Eigenschaften F1) und F2), nämlich 
 	\begin{equation}
 	\label{eq01}
 	\mathcal(P)\ni B\mapsto P_A(B):=P(B|A)
 	\end{equation}
 	
 	\paragraph{Beweis:}
 	\begin{itemize}
 		\item[1)] Wir verifizieren, dass Gleichung (\ref{eq01}) F1) \& F2 erfüllt:
 		\begin{itemize}
 			\item[F1)] $$P_A (A)=P(A|A)=\frac{P(A\cap A)}{P(A)}=1$$
 			\item[F2)] $$C\subset A\Rightarrow P_A(C)=P(C|A)=\frac{P(C\cap A)}{P(A)}=
 			\frac{1}{P(A)}\cdot P(A)$$
 			$\hookrightarrow$ F2) ist erfüllt mit $k_A = \frac{1}{P(A)}$ 
		\end{itemize}
		\item[2)] Sei $P_A$: $P(\Omega)\rightarrow[0,1]$ ein W-Maß mit F1) \& F2). Dann gilt:
		\begin{equation}
		\label{eq02}
		P_A \text{ F1) erfüllt }\Rightarrow P_A(A^C) = 1-P_A (A)=0
		\end{equation}
		nun ist $(A,A^C)$ eine Zerlegung von $\Omega$\\
		$\hookrightarrow$ Nun betrachten wir für die beliebige $B\subset \Omega$ die Zerlegung
		$$B=(B\cap A)\dot{\cup} (B\cap A^C)$$
		$$\hookrightarrow P_A (B)=P_A (\underbrace{B\cap A}_{\subset A}) + 
\underbrace{P(\underbrace{B\cap A^C}_{\subset A^C})}_{=0, \text{ wegen }(\ref{eq02})}$$
		\begin{equation}
		\label{eq03}
		\hookrightarrow P_A(B)=P_A(B\cap A) = k_A\cdot P(B\cap A)
		\end{equation}
		Setze in Gleichung (\ref{eq03}) : $B=A$
		$$\hookrightarrow 1=P_A (B=A)=k_A\cdot P(A\cap A)= k_A \cdot P(A)$$
		$$\Leftrightarrow k_A = \frac{1}{P(A)}$$	   		 
 	\end{itemize}
 	
 	\paragraph{Beispiel:} 
 	Ein Beutel mit 3 Münzen:
 	\begin{itemize}
 		\item eine gewöhnliche Münze: \textbf{KZ}
 		\item eine Münze: \textbf{KK}
 		\item eine Münze: \textbf{ZZ}
 	\end{itemize}
	Eine Münze wird zufällig gezogen und geworfen:
	
	\paragraph{Annahme:} 
	Ergebnis ist \say{Zahl} (oben zu sehen)
	
	\paragraph{Frage:}
	Was ist die Wahrscheinlichkeit dafür, dass auf der Rückseite (unten) \say{Kopf} liegt?\\
	Wir formulieren die relevanten Ereignisse.
	\begin{itemize}
		\item[A:] gezogene Münze ist ZZ
		\item[B:] gezogene Münze ist KK
		\item[C:] gezogene Münze ist KZ
		\item[D:] geworfene Münze zeigt nach oben \say{Z} (Zahl)
		\item[E:] geworfene Münze zeigt nach unten \say{K} (Kopf)
	\end{itemize}
	Mit dieser Bezeichnung lautet die gestellte Frage: $P(E|D)=$? \\
	Definitionsgemäß gilt: 
	$$P(E|D)=\frac{P(E\cap D)}{P(D)}$$
	\begin{align*}
	\hookrightarrow P(D)&= P(D|A)\cdot P(A) + P(D|B)\cdot P(B) + P(D|C)*P(C)\\
	 &= 1 \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + \cdot \frac{1}{2} \cdot\frac{1}{3}\\
	  &= \frac{1}{2}
	\end{align*}
	\begin{align*}
	P(D\cap E) &= P(D\cap E|A)\cdot P(A) + \cdots\\
	 &= 0 \cdot \frac{1}{3} + 0\cdot\frac{1}{3} + \frac{1}{2}\cdot\frac{1}{3}\\
	 &=\frac{1}{6}
	\end{align*}
	$$\hookrightarrow P(E|D) = \frac{\frac{1}{6}}{\frac{1}{2}}=\frac{1}{3}$$


\section{Gemeinsame Verteilung}
	\paragraph{Ausgangspunkt:}
	Es existiert ein diskreter W-Raum $(\Omega,\mathcal{P}(\Omega),P)$.\\
	Gegeben seien zwei Abbildungen $X$ und $Y$ auf $(\Omega,\mathcal{P}(\Omega),P)$.\\
	$\hookrightarrow$ Sie stellen Zufallsvariablen dar mit jeweils Werten in $X(\Omega):=\Omega_X$ und $Y(\Omega)=:\Omega_Y$, d.h.
	$$X:\Omega\rightarrow\Omega_X \text{ und } Y:\Omega\rightarrow\Omega_Y$$
	$X$ und $Y$ heißen unabhängige (siehe \ref{sec6.2}), wenn gilt:
	$$P(X\in A, Y\in B)=P(X\in A) \cdot P(Y\in B), \:\forall A\subset\Omega_X, \forall B\subset\Omega_Y$$	
	\begin{align*}
	\text{Rechte Seite: } & A\subset\Omega_X\rightarrow P(X\in A) \tilde{=} \text{ Verteilung von } X\\
	& B\subset\Omega_Y\rightarrow P(Y\in B) \tilde{=}  \text{ Verteilung von } Y
	\end{align*}
	\begin{align}
	\label{gemeinVerteilungGleichung}
	\text{Linke Seite: } &  (A\in\Omega_X , B\subset\Omega_Y)\rightarrow P(X\in A, Y\in B)
	\end{align}
	
			
	
	\paragraph{Frage:}
	Stellt die Abbildung (siehe Gleichung(\ref{gemeinVerteilungGleichung}) ebenfalls eine Verteilung/W-Maß dar?
	\paragraph{Antwort:}
	Das ist nicht der Fall, da der Definitionsbereich der Abbildung (Gleichung \ref{gemeinVerteilungGleichung}) zu klein ist. Er lässt sich nur mit einer echten Teilmenge der Potenzmenge des karthesischen Produktes $\Omega_X \times\Omega_Y$ identifizieren.
	\paragraph{Beispiel:}
	$\underbrace{\{1,2\}}_{=\Omega_M} \times \underbrace{\{1,...,6\}}_{=\Omega_W} \tilde{=}$ Ergebnismenge zum Zufallsexperiment,
	bei den eine Münze und ein Würfel geworfen werden. Betrachte das Ereignis:
	$$E:=\{(1,2)\: , \: (2,1)\}\subseteq\Omega_M \times \Omega_W$$
	$\hookrightarrow$ Offensichtlich existiert keine Teilmenge $A\subset\Omega_M$ und $B\subset\Omega_W$, so dass
	$$E=A\times B=\{(x,y)\: : \: x\in A, y\in B\}$$
	\paragraph{Diskussion:} Wir stellen eine minimale Forderung an die Erweiterung des Definitionsbereiches (aus Gleichung \ref{gemeinVerteilungGleichung}). \\
	$\hookrightarrow$ abstrakte Formulierungen:
	Sei $\Omega$ eine Menge und $A$ ein System von Ereignissen bzw. Teilmengen von $\Omega$. Wir fordern:
	\begin{enumerate}
		\item	$\Omega\in A$ 
		\item	$E\in A \Rightarrow E^\complement = \Omega\setminus E\in A$
		\item	$I$ eine abzählbare Indexmenge und $E_i \in A_i, i\in I$
				$$\Rightarrow\bigcup_{i\in I}E_i \in A_i$$
	\end{enumerate}
	Im obigen Beispiel: 
	\begin{align*}
	E=\{(1,2),(2,1)\} & =\{(1,2)\}\cup\{(2,1)\}\\
	& =\underbrace{\{(1)\}}_{=\Omega_M} \times \underbrace{\{(2)\}}_{=\Omega_W}
	\end{align*}
	Forderung (1)-(3) auf dem fiktivem System von Ereignissen in $\Omega_X \times\Omega_Y$, welches mit der Potenzmenge $\mathcal{P}(\Omega_X \times \Omega_Y)$ übereinstimmt.
	
	\subsection{Definition Gemeinsame Verteilung}
	Seien $X$ und $Y$ zwei Zufallsvariablen auf einem gemeinsamen diskreten W-Raum\\ $(\Omega,\mathcal{P}(\Omega),P)$ mit Werten in $\Omega_X$ und $\Omega_Y$. Die \textit{gemeinsame Verteilung} von $X$ und $Y$ ist definiert als das W-Maß auf $\mathcal{P}(\Omega_X \times \Omega_Y)$ welches durch
	$$E\in\mathcal{P}(\Omega_X \times \Omega_Y)\longmapsto P((X,Y)\in E)$$
	gegeben ist. Wir bezeichnen es mit $P_{(X,Y)}$.
	Beachte: 
	$$P_{(X,Y)}(E)=P((X,Y)\in E)= P(\{\omega\in\Omega: \: (X(\omega),Y(\omega))\in E\}$$
	\paragraph{Bemerkung:}
	Man kann zeigen, dass es eine eindeutige Fortsetzung der Abbildung (Gleichung \ref{gemeinVerteilungGleichung}) zu einem W-Maß auf $\mathcal{P}(\Omega_X \times \Omega_Y)$ gibt und diese dann mit der gemeinsamen Verteilung von $X$ und $Y$ übereinstimmt.
	
	\subsection{Definition Produktmaß}
	Seien $Q_1$ und $Q_2$ zwei diskrete W-Maße jeweils auf der Potenzmenge $\mathcal{P}(\Omega_1)$ und $\mathcal{P}(\Omega_2)$. Dann heißt das W-Maß $P$ auf $\mathcal{P}(\Omega_1 \times \Omega_2)$ mit der Eigenschaft:
	$$P(E_1 \times E_2)=Q_1(E_1)\cdot Q_2(E_2),\: \forall E_i\in\mathcal{P}(\Omega_i),\: i=1,2$$
	das \textit{Produktmaß} von $Q_1$ und $Q_2$ und wird mit $Q_1 \otimes Q_2$ bezeichnet. 
	
	\subsection{Satz Unabhängige Familie}
	Seien $n\in\mathbb{N}$ , $n\geq2$. Die gemeinsame Verteilung einer Familie $X_i$ , $i=1,...,n$, von Zufallsvariablen jeweils mit der Verteilung $P_X$ ist gleich dem Produktmaß 
	$$\bigotimes_{i=1}^n P_{X_i}$$ 
	genau dann wenn die Familie \textit{unabhängig} ist.\\
	Die Gewichtsfunktion $\rho$ der gemeinsamen Verteilung $P_{X_1, ..., X_n}$ einer unabhängigen Familie ist gleich dem Produkt der Gewichtsfunktion $\rho_i$ von $X_i$ , $i=1,...,n$, d.h.
	$$\rho (x_1,...,x_n)=\prod_{i=1}^n \rho_i (x_i) \quad , \: \text{ für jedes } (x_1,...,x_n)\in\Omega_1 \times ... \times \Omega_n$$
	\paragraph{Beispiel:} 
	$$\rho(x_1,...,x_n)=\frac{1}{6^n}=\frac{1}{6}\cdot ... \cdot \frac{1}{6}=\rho(x_1)\cdot ... \cdot \rho(x_n)$$
	als Gewichtsfunktion der Verteilung $P_{(X,Y)}$ wobei 
	$$X \text{ : Augenzahl bei } i \text{ Versuchen}$$
	
	
	\paragraph{Annahme:}
	$X$ und $Y$ sind \textbf{nicht} unabhängig!\\
	$\hookrightarrow$ Wir benutzen das Konzept der bedingten Wahrscheinlichkeit/W-Maße:\\
	Für jedes $A\in\mathcal{P}(\Omega_X)$ mit $P_X(A)>0$ gilt
	$$P_{(X,Y)}(A\times B)=\underbrace{P_X(A)}_{=P_X(A)}\cdot P(Y\in B | X\in A)$$
	
	\paragraph{``Beweis'':}
	\begin{align*}
	\text{Linke Seite} &= P\left(X^{-1}(A)\cap Y^{-1}(B)\right)\\
	\text{Rechte Seite} &= P\left(X^{-1}(A)\right)\cdot
\frac{P\left(Y^{-1}(B)\cap X^{-1}(A)\right)}{P\left(X^{-1}(A)\right)}
	\end{align*}

\pagebreak	
\section{Das schwache Gesetz der großen Zahlen (GGZ)}
	\paragraph{Ziel:}
	Das Langzeitverhalten von Mittelwerten erfassen
	\begin{center}
	\textbf{Bernoulli's GGZ}
	\end{center}
	$\hookrightarrow$ Es beschreibt das Verhalten der Langzeitmittelwerte von Beobachtungswerten, die man bei unabhängig durchgeführten Bernoulli-Experimenten erhält.
	\paragraph{Genauer:}
	Seien $p\in(0,1)$ und $X\sim B_p$, d.h. $X$ ist Bernoulli-verteilt mit Erfolgswahrscheinlichkeit p\\
	$\hookrightarrow$ Beispiel Münzwurf $\rightarrow$ bei fairer Münze ist $p=\frac{1}{2}$\\
	Sei $n\in\mathbb{N}$. Es werden unabhängig voneinander $n$ Bernoulli-Experimente durchgeführt, die jeweils die Zufallsvariablen 
	$$X_i\sim B_p \: , \quad i=1,...,n$$
	beschrieben werden.\\
	$\hookrightarrow$ Wir interessieren uns für die
	\begin{itemize}
		\item $\underbrace{\text{absolute Häufigkeit}}_{\tilde{=} \text{ zufällige Anzahl der Treffer}}$ der $\underbrace{\text{Treffer}}_{\tilde{=}X_i =1}$ bei $n$ Einzelexperimenten.
		$$\hookrightarrow S_n = X_1 + ... + X_n$$
		\item beziehungsweise die relative Häufigkeit der Treffer $\tilde{=} 
\frac{1}{n}S_n =: \bar{S_n}$
	\end{itemize}
	Wir erinnern uns: $S_n \sim B_{n,p}$ , d.h. $S_n$ ist binomialverteilt zu den Parametern $n$ und $p$.
	\paragraph{Ziel:}
	Das Verhalten der relativen Häufigkeit der Treffer mit wachsender Anzahl $n$ der Versuche zu erfassen.
	
	\begin{itemize}
		\item[$\hookrightarrow$] \textbf{Frage:} $n=10000$ Versuche mit einer fairen Münze\\
		$\hookrightarrow$ Wie viele Treffer?\\
		$\rightarrow$ \textbf{Antwort:} ungefähr $5000 \tilde{=} \frac{1}{2}$ als relative Häufigkeit der Treffer.
		\item[$\hookrightarrow$] Variante: mit einer unfairen Münze mit Erfolgswahrscheinlichkeit $p\left(\neq\frac{1}{2}\right)$\\
		$\hookrightarrow$ \textbf{Frage:} Wie hoch ist die relative Häufigkeit der Treffer?\\
		\textbf{Antwort:} ungefähr $p$ 
	\end{itemize}
	
	Es ist noch nicht klar, wie wir unsere Antwort formal darstellen können. Beispielsweise führt folgender naheliegender Ansatz nicht zum Ziel.
	$$P\left(\left\{ \frac{1}{2n}\sum_{i=1}^{2n}X_i = \frac{1}{2}\right\}\right) = B_{2n, \frac{1}{2}}(\{n\})$$
	$$=\binom{2n}{n}\cdot \frac{1}{2^{2n}}\rightarrow_{n\rightarrow\infty} 0$$
	
	\subsection{Satz Bernoullis GGZ}
	\label{bernoulliGGZ}
	Sei $p\in(0,1)$. Seien $X_i \sim B_p$ , $i\in\mathbb{N}$ und 
	$$\bar{S_n}:=\frac{1}{2}\sum_{i=1}^{n}X_i \: , \quad n\in\mathbb{N}$$
	Dann gilt für ein beliebiges $\varepsilon >0$:
	$$\lim_{n\to\infty} P\left(\left| \bar{S_n}-p\right|\geq\epsilon \right)=0$$	
	Folgende Resultate gehen in den Beweis von Satz \ref{bernoulliGGZ} ein.
	\begin{enumerate}
		\item $$\mathbb{E}(\bar{S_n})=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n X_i\right)= \frac{1}{n}\cdot np= p$$
		\item $$Var(\bar{S_n})=Var\left(\frac{1}{n}S_n\right)=\frac{p(1-p)}{n}$$
		\item Tschebyscheff-Ungleichung
	\end{enumerate}	 
	
	\subsection{Lemma Rechenregeln für die Varianz}
	Sei $X$ eine reelle, diskrete Zufallsvariable mit $\mathbb{E}(X)=m$ und $Var(X)=v$. Dann gilt für alle $a,b\in\mathbb{R}$:
	$$Var(aX+b)=a^2\cdot Var(X)$$
	\paragraph{Beweis:}
	
	\begin{align*}
	Var(aX) &=\mathbb{E}\left(\left(aX\right)^2\right)-\left(\mathbb{E}\left(aX\right)\right)^2\\
	\text{Liniarität des Erwartungswertes (siehe 5.6 } &= a^2\mathbb{E}(X^2)-(a\mathbb{E}(X))^2\\
	&= a^2\cdot (\mathbb{E}(X^2)-(\mathbb{E}(X)^2))=a^2\cdot Var(X)
	\end{align*}
	
	\begin{align*}
	\text{zu zeigen: } Var(x+b)&=Var(X)\\
	Var(x+b) &=\mathbb{E}(\underbrace{(X+b)^2}_{X^2+2bX+b^2})-\underbrace{(\mathbb{E}(X+b))^2}_{(\mathbb{E}(X)+b)^2}\\
	&= \mathbb{E}(X^2)+2b\mathbb{E}(X)+b^2-\left(\left(\mathbb{E}\left(X\right)\right)^2+2b\mathbb{E}\left(X\right)+b^2\right)
	\end{align*}		

	\subsection{Lemma Tschebyscheff-Menge}
	Sei $X$ eine reelle, diskrete Zufallsvariable mit $\mathbb{E}(X)=,$ und der Varianz $Var(X)=v$. Dann gilt für jedes $\varepsilon >0$:
	$$P\left(|X-m|\geq\varepsilon\right)\leq\frac{Var(X)}{\varepsilon^2}$$
	\paragraph{Beweis:}
	Es ist zu zeigen $Var(X)\geq\varepsilon^2\cdot P(|X-m|\geq\varepsilon)$
	Sei $\tilde{X}:=(X-m)^2$ wobei zu beachten ist, dass $\tilde{X}\geq 0$
	$$Var(X)=\mathbb{E}(\tilde{X})=\sum_{\tilde{x}\in\tilde{\Omega}}\tilde{x} \cdot 
P_{\tilde{X}}(\{\tilde{x}\})\geq\sum_{\tilde{x}\in M} \tilde{x}P_{\tilde{X}}(\{\tilde{x}\})$$
	wobei $M\subseteq\tilde{\Omega}$, speziell $M:=\{\tilde{X}\geq\varepsilon^2\}$ ist. Dann gilt demzufolge
	$$\sum_{\tilde{x}\in M} \tilde{x}P_{\tilde{X}}(\{\tilde{x}\})=\sum_{\tilde{x}\in M} \varepsilon^2 \cdot P_{\tilde{X}}(\{\tilde{x}\})=\varepsilon^2 \cdot \underbrace{P\left(\left(X-m\right)^2\geq\varepsilon^2\right)}_{=|X-m|>\varepsilon}$$
	\paragraph{Bemerkung:}
	$$P\left(\left| X-m\right| < \varepsilon\right)\geq 1-\frac{Var(X)}{\varepsilon^2}$$ 
	
	\paragraph{Beweis zu Satz \ref{bernoulliGGZ}:}
	schwache GGZ für Bernoulli-Zufallsvariablen.\\
	Für $n\in\mathbb{N}$ ist $\bar{S}_n = \frac{1}{n}\sum_{i=1}^n X_i$ und es gilt:

	\begin{align*}
	P\left(\left| \bar{S}_n -p\right|\geq\varepsilon\right) &= P\left(\left|\bar{S}_n -\mathbb{E}\left(\bar{S}_n\right)\right|\geq\varepsilon\right)\\
	&\leq \frac{Var(\bar{S}_n)}{\varepsilon^2}\\
	&= \frac{\frac{1}{n^2} Var(S_n)}{\varepsilon^2}\\
	&= \frac{p\cdot(1-p)}{n\cdot \varepsilon^2}\lim_{n\rightarrow\infty}
	\end{align*}		
	
	\paragraph{Bemerkung zu vorherigen Beweis:}
	$$\mathbb{E}\left(\bar{S}_n \right) = \frac{1}{n}\sum_{i=1}^n \underbrace{\mathbb{E}(X_i)}_{=p}=p$$
	
	\subsection{Satz}
	Seien $m\in\mathbb{R}$ und $0<v<+\infty$. Sei $X_i$ , $i\in\mathbb{N}$ eine Folge von Zufallszahlen so dass
	$$\mathbb{E}(X_i)=m\: ,\quad \forall i\in\mathbb{N} \text{ und }$$
	$$Var(X_i)=v\: , \quad \forall i \in\mathbb{N}$$
	Dann gilt für jedes $\varepsilon>0$:
	$$\lim_{n\rightarrow\infty} P\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - m\right|\leq\varepsilon\right)=0$$
	
	\subsection{Lemma}
	\label{lemma9.5}
	Seien $n\in\mathbb{N}$ und $X_i$ , $i=1,...,n$ paarweise unabhängige reelle (diskrete) Zufallsvariablen jeweils mit dem Erwartungswert $\mathbb{E}(X_i)=m_i\in\mathbb{R}$ und der Varianz $Var(X_i)=v_i<+\infty$. Dann gilt:
	$$Var\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n Var(X_i) = \sum_{i=1}^n v_i$$
	
	\subsection{Definition Unkorrelation \& Kovarianz}
	Zwei Zufallsvariablen $X$ und $Y$ heißen \textit{unkorreliert}, wenn die sogenannte  \textit{Kovarianz}:
	$$Cov(X,Y):=\mathbb{E}\left(\left(X-\mathbb{E}\left(X\right)\right)\cdot \left(Y-\mathbb{E}\left(Y\right)\right)\right)$$
	verschwindet. Das heißt, dass $Cov(X,Y)=0$
	\paragraph{Bemerkung:}
	\begin{enumerate}
		\item $Cov(X,X)=\mathbb{E}((X-\mathbb{E}(X))^2) = Var(X)$
		\item $X$ und $Y$ unabhängig $\Rightarrow$ $X$ und $Y$ unkorreliert (Rückimplikation gilt nicht!)
	\end{enumerate}
	
	\subsection{Satz Gleichheit von Bienaymé}
	Seien $X_i$ , $i=1,...,n$ paarweise unkorrelierte, reelle und diskrete Zufallsvariablen, dann gilt Satz \ref{lemma9.5} .
	\paragraph{Beweis:}
	
	\begin{align*}
	Var\left(\sum_{i=1}^n \tilde{X}_i\right) &= Var\left(\sum_{i=1}^n \tilde{X}_i\right)\\
	&= \mathbb{E}\left(\left(\sum_{i=1}^n \tilde{X}_i \right)^2 \right)\\
	&= \mathbb{E}\left(\sum_{i,j=1}^n \tilde{X}_i \cdot \tilde{X}_j\right)\\
	&= \mathbb{E}\left(\sum_{i=1}^n \tilde{X}_i^2 + \sum_{i,j=1\: , \: i\neq j}^n  \tilde{X}_i \tilde{X}_j\right)\\
	&= \sum_{i=1}^n \underbrace{\mathbb{E}\left(\tilde{X}_i^2\right)}_{Var(\tilde{X}_i} + \mathbb{E}\left(\sum_{i=1\: , \: i\neq j} \tilde{X}_i \tilde{X}_j\right)\\
	&= \sum_{i=1}^n Var(\tilde{X}) + \sum_{i=1\: , \: i\neq j}\mathbb{E}\left(\tilde{X}_i \tilde{X}_j\right)\\
	&= \sum_{i=1}^n Var(X_i) 
	\end{align*}
	 
	\section{Reelle Zufallsvariablen}
	Bis jetzt wurden folgendes betrachtet:\\
	Die diskreten Spezialfälle:
	\begin{itemize}
		\item Zufallsexperiment mit \emph{höchstens abzählbarer} Ergebnismenge
		\item W-Räume der Gestalt $(\Omega, \mathcal{P}(\Omega),P)$, wobei $\Omega$ \emph{höchstens abzählbar}
		\item Zufallsvariablen mit \emph{höchstens abzählbaren} Wertebereich
	\end{itemize}
	
	\subsection{Definition Sigma-Algebra}
	Sei $\Omega$ eine beliebige nicht-leere Menge. Ein System $\mathcal{A}\subseteq\mathcal{P}(\Omega)$ von Teilmengen von $\Omega$ heißt eine \textit{$\sigma$-Algebra in $\Omega$} und Paar $(\Omega,\mathcal{A})$ ein Ereignisraum, wenn $\mathcal{A}$ folgendes erfüllt:
	\begin{itemize}
		\item[S1] $\Omega\in\mathcal{A}$
		\item[S2] $A\in\mathcal{A}\: \Rightarrow$ $A^\complement \in \mathcal{A}$
		\item[S3] $\displaystyle \bigcup_{i\in\mathbb{N}} A_i \: \in\mathcal{A}$, wenn $A_i\in\mathcal{A}$ , $i\in\mathbb{N}$ 
	\end{itemize}
	
	Jedes System $\mathcal{G}$ von Teilmengen von $\Omega$ kann zu einer $\sigma$-Algebra erweitert werden. (trivial $\mathcal{G}\rightarrow\mathcal{P}(\Omega)$)\\
	$\hookrightarrow$ wichtig: Zu jedem $\mathcal{G}$ von Teilmengen von $\Omega$ existiert genau eine kleinste $\sigma$-Algebra, die von $\mathcal{G}$ erzeugt wird.	Sie wird als die von $\mathcal{G}$ erzeugte $\sigma$-Algebra bezeichnet. \\
	$\hookrightarrow$ Notation: $\sigma(\mathcal{G})$
	
	\paragraph{Beispiele:}
	\begin{enumerate}
	\label{enum10.1}
		\item Sei $\mathcal{G}:=\{\Omega\}\Rightarrow \: \sigma(\mathcal{G})=\{\Omega,\varnothing\}$
		\item Sei $\Omega=\mathbb{R}$ und $\mathcal{G}:=\{[-1,1]\}\Rightarrow\: \sigma(\mathcal{G})=\{[-1,1],\mathbb{R}\backslash [-1,1],\mathbb{R},\varnothing\}$
		\item $\Omega$ eine abzählbare Menge, $\mathcal{G}:=\{\{\omega\}:\: \omega\in\Omega\}\Rightarrow \: \sigma(\mathcal{G})=\mathcal{P}(\Omega)$
	\end{enumerate}
	
	\subsection{Definition Borelsche Sigma-Algebra}
	\label{defBorelsch}
	Sei $\Omega = \mathbb{R}$ und $\displaystyle \mathcal{G}:=\{[a,b]: \: a,b\in\mathbb{Q}$ und $a<b\}$. Dann heißt $\sigma (\mathcal{G})$ die \textit{Borelsche} $\sigma $-Algebra auf $\mathbb{R}$ und wird mit $\mathcal{B}$ oder $B(\mathbb{R})$ bezeichnet.	
	
	\paragraph{Bemerkung:}
	\begin{itemize}
		\item $\displaystyle B^1 \subsetneq \mathcal{P}(\mathbb{R})$
		\item $\displaystyle \tilde{\mathcal{G}}:= \left\{(-\infty,c]: \: c\in\mathbb{R}\right\} \Rightarrow \sigma(\tilde{\mathcal{G}})=\sigma(\mathcal{G})=B^1$
	\end{itemize}
	
	
	\subsection{Definition }
	Seien $\Omega$ eine beliebige nicht-leere Menge und $\mathcal{A}$ eine $\sigma$-Algebra in $\Omega$. Eine Abbildung $P:\mathcal{A}\rightarrow[0,1]$ heißt W-Maß auf $\mathcal{A}$, wenn sie folgende Eigenschaften besitzt:
	\begin{itemize}
		\item[W1] $P(\Omega) = 1$ \quad (Normierung)
		\item[W2] $\displaystyle P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)$ wenn: 
		\begin{itemize}
			\item $A_i\in\mathcal{A}$ , $i\in\mathbb{N}$ und
			\item $A_i\cap A_j =\varnothing$ für $i\neq j$
		\end{itemize}
	\end{itemize}
	Wichtige Eigenschaften von W-Maßen ( analog zum diskreten Fall):
	\begin{enumerate}[i]
		\item $P(\varnothing)=0$
		\item $P(A^\complement )= 1-P(A)$ , $\forall A\in \mathcal{A}$
		\item $P(A)\leq P(B)$ wenn $A\subseteq B$ \quad $(\forall A,B\in\mathcal{A})$\\
		( Monotonie )
	\end{enumerate}
	\paragraph{$\hookrightarrow$ Frage:}
	Gibt es Unterschiede zum diskreten Fall?
	\paragraph{Diskussion:}
	Verallgemeinerung des Konzeptes Zähldichte/Gewichtsfunktion?
	\paragraph{Antwort:}
	`` Existiert nicht immer ''
	
	\paragraph{Betrachtung von W-Maßen auf $\mathcal{B}^1$:}
	\begin{itemize}
		\item \textit{Lebesgue}-integrierbare Funktion $f:\: \mathbb{R}\rightarrow[0,1]$ mit
		$$\int f(x) dx = 1$$
		stellt eine Dichte von einem W-Maß dar:
		$$P(E):=\int_E f(x) dx \quad , \quad \forall E\in \mathcal{B}^1$$
		\item Standardbeispiel eines W-Maßes auf $\mathcal{B}^1$ , welches keine Dichte besitzt:
		\[
 	\delta_{x_o}: \: \mathcal{B}^1\rightarrow [0,1] \quad \sigma_{x_o}(E)=\left\{\begin{array}{lr}
 		1 &, x_o\in E\\
 		0 &,\text{sonst }
 	   \end{array}\right.
 		\]
			
	\end{itemize}
	
	\subsection{Definition Reeller Wahrscheinlichkeitsraum}
	Seien $\Omega$ eine beliebige Menge $(\Omega\neq\varnothing)$ , $\mathcal{A}$ eine $\sigma$-Algebra in $\Omega$ und $P: \mathcal{A}\rightarrow[0,1]$ ein W-Maß, dann heißt $(\Omega, \mathcal{A},P)$ ein \textit{W-Raum}.\\
	Eine reelle Zufallsvariable $X$ nimmt Werte in $\Omega=\mathbb{R}$ beziehungsweise in einer Borelschen Menge $\Omega\in\mathcal{B}^1$ an.
	Man wird insbesondere Ereignisse wie
	$$\left\{ X\leq c\right\} \: , \: \{X\geq c\} \: , \: \{X\in J\}$$
	eine Wahrscheinlichkeit zuordnen wollen, wobei $c\in\mathbb{R}$ , $J$ Intervall in $\mathbb{R}$.\\
	$\hookrightarrow$ Solche Ereignisse bilden gerade das Mengensystem $\tilde{G}$, welches die Borelsche $\sigma$-Algebra $\mathcal{B}^1$ erzeugt.
	
	\subsection{Definition Reelle Zufallsvariable}
	Sei $(\Omega ,\mathcal{A}, P)$ ein W-Raum. Eine Abbildung:
	$$X:\Omega\rightarrow\mathbb{R}$$
	heißt eine \textit{reelle Zufallsvariable}, wenn gilt:
	$$X^{-1}\left((-\infty, c]\right) \in \mathcal{A} \: , \quad \forall c \in\mathbb{R}$$
	Die Verteilung $P_X$ von $X$ ist definiert durch
	$$P_X(E):= P\left(X^{-1} (E)\right) \quad \forall E\in\mathcal{B}^-1$$
	
	\subsection{Definition Verteilungsfunktion}
	Sei $P$ ein W-Maß auf $\mathcal{B}^1$. Die Funktion 
	$$F:\mathbb{R}\rightarrow\mathbb{R} \: , \quad c\longmapsto P\left((-\infty,c]\right)$$
	heißt \textit{Verteilungsfunktion} von $P$. Für eine reelle Zufallsvariable $X$ ist die Verteilungsfunktion $F_X$ definiert durch:
	$$F_X:\mathbb{R}\rightarrow[0,1], \quad c\longmapsto F_X(c):=P(X\subseteq c)$$
	
	\subsection{Definition Stetige Gleichverteilung}
	\label{defStetigeGleichverteilung}	
	Sei $M\in\mathcal{B}^1$ (eine borelsche Menge) mit $\lambda^1 (M)=m$ und 
	$$\rho : M\rightarrow\mathbb{R}\: , \quad x\longmapsto \frac{1}{m}$$
	eine konsistente Funktion. Ein W-Maß auf $(M,\mathcal{B}^1_M )$ mit der Dichtefunktion $\rho$ heißt dann \textit{stetige Gleichverteilung} auf $M$.
	\paragraph{Notation:} $\mathcal{U}_M$
	
	\subsection{Definition Normalverteilung}
	\label{defNormalverteilung}
	Ein W-Maß auf $\mathcal{B}^1$ mit der Dichtefunktion
	$$\phi_{m,v} : \mathbb{R}\rightarrow\mathbb{R} \: , \quad x\longmapsto \frac{1}{\sqrt{2\pi v}}e^{-\frac{(x-m)^2}{2v}}$$
	heißt \textit{Normalverteilung} oder \textit{Gauß-Verteilung} mit der Varianz $v$.
	\paragraph{Notation:} $\mathcal{N}_{m,v}$
	\paragraph{Bemerkung:}
	$\mathcal{N}_{m=0,v=1}$ heißt \textit{Standardabweichung}.


	\pagebreak	
	\section{Zentraler Grenzwertsatz}
	$$P\left(\left| \frac{1}{n}\sum_{i=1}^n X_i -p \right| <\epsilon\right) \xrightarrow[n\rightarrow\infty]{}1$$
	$$\Leftrightarrow \:\: P\left( \left| \sum_{i=1}^n X_i -np\right| > \epsilon \cdot n \right) \xrightarrow[n\to\infty]{} 1$$	
	\subsection{Satz von Moivre-Laplace}
	Seien $X_i$ , $i\in\mathbb{N}$ paarweise unabhängige, Bernoulli-verteilte Zufallsvariablen jeweils mit der Erfolgswahrscheinlichkeit $p\in(0,1)$ , das heißt:
	$$X_i \sim \mathcal{B}_p \: , \: \forall i\in\mathbb{N}$$
	Dann gilt für alle $a,b\in\mathbb{R}$ mit $a<b$:
	\begin{equation}
	\label{satzMoivreLaplaceEq01}
	\lim_{n\to\infty} P(a\leq S_n^{*} \leq b) = \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{x^2}{2}} dx
	\end{equation}
	wobei
	$$S_n^{*} := \frac{S_n - np}{\sqrt{n\cdot p \cdot (1-p)}} := \frac{\sum_{i=1}^n X_i - np}{\sqrt{n\cdot p \cdot (1-p)}}$$
	\paragraph{Diskussion:}
	\begin{itemize}
		\item \textbf{Linke Seite} von Gleichung \ref{satzMoivreLaplaceEq01} : \\
		$S_n \sim \mathcal{B}_{n,p} \Rightarrow$
		\begin{enumerate}
			\item $\mathbb{E}(S_n)=n\cdot p$
			\item $Var(S_n) = n \cdot p \cdot (1-p)$
		\end{enumerate}
		$\hookrightarrow \mathbb{E}(S_n^{*})=0$ und $Var(S_n^{*})=1$\quad \footnote{$Var(aX)=a^2\cdot Var X$}
		\item \textbf{Rechte Seite} von Gleichung \ref{satzMoivreLaplaceEq01} : \\
		$\displaystyle x\longmapsto \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} = \phi_{0,1}(x)$  ... Dichtefunktion der Standardnormalverteilung $\mathcal{N}_{0,1}$
		$\displaystyle \hookrightarrow \underbrace{\int_a^b \phi_{0,1} (x)dx}_{=F_X(b)-F_X(a)} = P(a\leq X \leq b)$, wenn $X\sim\mathcal{N}_{0,1}$
	\end{itemize}

	Der Satz von Moivre-Laplace begründet die Normalapproximation der Binomialverteilung. Die Zähldichte der Binomialverteilung lässt sich für große $n$ ``gut'' durch die Dichtefunktion der Normalverteilung approximieren.\\
	\textit{Standardisierung}, das heißt der Übergang von $S_n \rightarrow S_n^{*}$ bewirkt 
	$$h_{n,p}^{*} (l) := \sqrt{n\cdot p \cdot (1-p)} \mathcal{B}_{n,p}(\{l\cdot \sqrt{n\cdot p\cdot (1-p)} -np\})$$
	Anders ausgedrückt
	$$k=0,1,...m, \longmapsto l(k) = \frac{k-np}{\sqrt{n\cdot p(1-p)}}$$
	$$\text{und } \: h_{n,p}(k) = \mathcal{B}_{n,p} (\{k\}) \longmapsto h_{n,p}^{*}(\{l(k)\})$$
	$\hookrightarrow$ Ziel: Verallgemeinerung des Satzes von Moivre-Laplace. Diese basiert auf dem Konzept der \textit{Konvergenz in Verteilung}.
	
	\subsection{Definition Konvergenz in Verteilung}
	Sei $X_i$ , $i\in\mathbb{N}$ eine Folge von reellen Zufallsvariablen $X_i$ jeweils mit der Verteilungsfunktion $F_{X_i}$. Die Folge \textbf{konvergiert in Verteilung} gegen eine reelle Zufallsvariable $X$ mit Verteilungsfunktion $F_X$, wenn
	\begin{center}
	$\lim_{i\to\infty} F_{X_i}(c)=F_X(c)$ an allen Stetigkeitsstellen $c$ von $F_X$. 	
	\end{center}
	
	\paragraph{Notation:} $X_i \xrightarrow[]{d} X \quad (i\to\infty)$
	
	\subsection{Satz}
	Sei $X_i$ , $i\in\mathbb{N}$ unabhängige, identisch verteilte Zufallsvariablen mit
	\begin{enumerate}
		\item $\mathbb{E}(X_i) = m \quad (\forall i\in\mathbb{N})$ und 
		\item $Var(X_i)=v<+\infty \quad (\forall i\in\mathbb{N})$
	\end{enumerate}	 
	Dann gilt
	$$S_n^{*} := \frac{1}{\sqrt{n}} \sum_{i=1}^n \frac{X_i -m}{\sqrt{v}} \xrightarrow[]{d} X\sim\mathcal{N}_{0,1}$$
	
	\pagebreak
	
	\part{Addendum}
	
	\section*{Mengensysteme}
	Sein $\Omega$ eine beliebige Menge ($\Omega \neq \varnothing$): $\displaystyle G\subset \mathcal{P}(\Omega) \Rightarrow \sigma(G) = \cap_{G\subseteq \mathcal{A}} \mathcal{A}$
	\begin{enumerate}
		\item $\displaystyle G:=\{ \Omega \} \Rightarrow \sigma(G)=\{\Omega ,\varnothing\}$
		\item - 4. siehe Beispiele in \ref{enum10.1} 
	\end{enumerate}
	\paragraph{Beispiel (5):}
	Sei $\Omega\neq\varnothing$ beliebig. Betrachte $G:=\{ M_1 , ..., M_n\}$ eine disjunkte Zerlegung von $\Omega$ in nicht-leere Teilmengen, d.h.
	$$\bigcup_{i=1}^n M_i = \Omega$$
	Dann gilt: $\displaystyle \left| \sigma(G)\right| = 2^n$
	\paragraph{Begründung:}
	$$f:G \rightarrow M \: , \: M_i \longmapsto f(M_i):=m_i \text{ wobei } M:=\{m_1,...,m_n\}$$
	$$\left\{\underbrace{f(E)}_{\subseteq M} : \: E\in\sigma (G)\right\} \subseteq \mathcal P(\Omega)$$	
	Umgekehrt gilt: $\displaystyle \left\{f^{-1}(K): \: K\in\mathcal{P}(M)\right\}\subseteq \sigma(G)$ \\
	$P(M)$ und $\sigma(G)$ können bijektiv aufeinander abgebildet werden. \\
	$\Rightarrow \quad \mathcal{P}(M)$ und $\sigma (G)$ gleichmächtig, d.h. $|\sigma (G)|=|\mathcal{P}(G)|=2^{|M|}=2^n$
	\paragraph{Beispiel (6):}	
	Siehe Unterpunkt \ref{defBorelsch} .
	
	\section*{Dichtefunktion}
	Die sogenannte Dichte/Dichtefunktion auf $\mathbb{R}$ beziehungsweise $D\in\mathcal{B}^1$ stellt eine Analogie zur Zähldichte dar. Mit Dichtefunktionen lassen sich W-Maße auf $\mathcal{B}^1$ beziehungsweise $\mathcal{B}^1_D$ definieren
	$$\mathcal{B}^1_D = \{E\cap D : E\in\mathcal{B}^1\}$$
	$$$$
	\begin{align*}
	f:\: D\rightarrow\mathbb{R} \text{ Dichtefunktion }  \Leftrightarrow & (1)\:\: f(x)\geq 0 \quad \forall x\in D\\
	& (2)\:\: f^1(E)\in\mathcal{B}^1_D \quad \forall E\in\mathcal{B}^1\\
	& (3)\:\: \int fdx = 1
	\end{align*}
	
	\paragraph{$\rightarrow$ Spezialfall:}
	\begin{itemize}
		\item $\displaystyle D=[a,b]\subseteq\mathbb{R} \: (\in\mathbb{B}^1)$ ein Intervall
		\item $f:D\rightarrow\mathbb{R}$ positiv und stetig!
		\item[$\hookrightarrow$] $\displaystyle \int^b_a f(x)dx$ kann berechnet werden\\
		falls $\displaystyle \int_a^b f(x)dx=1$ , liegt mit $f$ eine Dichtefunktion vor
		\item[$\hookrightarrow$] Wir können ein W-Maß berechnen
		$$P([c,d])=\int_c^d f(x)dx \: , \quad c<d ,\:\: c,d\in D=[a,b]$$
		und allgemein $\displaystyle P(E)=\int_E fd(x)$, $\forall E\in\mathcal{B}^1_D$ (wobei hier Lebesgue-Integral auftaucht) 
	\end{itemize}

	\paragraph{Beispiel:}
	$D=[0,1]$ , $f(x)=2x$ \quad $(x\in[0,1])$
	
	\section*{Zusammenhang zwischen Verteilungsfunktion und Dichtefunktion}
	Sei $X$ eine reelle Zufallsvariable auf einem W-Raum $(\Omega, \mathcal{A}, P)$.\\
	Die Verteilung $P_X =P\circ X^{-1}$ von $X$ besitzt genau dann eine Dichtefunktion $\rho: \mathbb{R}\rightarrow\mathbb{R}$, wenn 
	$$\int_{-\infty}^c \rho (x)dx = F_X(c), \quad \forall c\in\mathbb{R}$$
	gilt, wobei $F_X$ die Verteilungsfunktion von $X$ bezeichnet.	
	
	\paragraph{Bemerkung:}
	Die Dichtefunktion $f$ der Verteilung $P_X$ ist genau dann eine stetige Funktion, wenn die Verteilungsfunktion $F_X$ von $X$ stetig differenzierbar ist. Dann gilt:
	$$f=F^1_X$$
	
	\paragraph{$\hookrightarrow$ Beispiele für stetige Funktionen}
	\begin{enumerate}
		\item Siehe Definition Gleichverteilung (\ref{defStetigeGleichverteilung})
		\item Siehe Definition Normalverteilung (\ref{defNormalverteilung})
	\end{enumerate}		
	
	
	
\end{document}
